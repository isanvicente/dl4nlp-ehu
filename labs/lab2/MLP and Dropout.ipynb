{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: MLPs and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6920\n",
      "Dev size: 872\n",
      "Test size: 1821\n"
     ]
    }
   ],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt') \n",
    "\n",
    "print('Training size: {}'.format(len(training_set)))\n",
    "print('Dev size: {}'.format(len(dev_set)))\n",
    "print('Test size: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 10 times in the training set, leaving us with $|V|=1254$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1254\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "                                \n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            # Extract features (by name) for one example\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features, dim\n",
    "    \n",
    "indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n",
    "\n",
    "print('Vocabulary size: {}'.format(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a batch evalution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow.\n",
    "\n",
    "### Part One:\n",
    "\n",
    "Modify it to turn it into an MLP with two ReLU hidden layers of 50 dimensions.\n",
    "\n",
    "Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. (Why?) You should use tf.random_normal instead, with stddev=0.1.\n",
    "\n",
    "If your model works, it should be able to overfit, reaching about 90% accuracy *on the training set* in the first 100 epochs.\n",
    "\n",
    "### Part Two:\n",
    "\n",
    "After each hidden layer, add dropout with a 80% keep rate. You're welcome to use `tf.nn.dropout`.\n",
    "\n",
    "Remember that dropout behaves differently at training time and at test time. This is not automatic. You can implement in various ways, but an easy way can be this:\n",
    "\n",
    "- Hint: Treat the keep rate as an input to the model, just like `x`. At training time, feed it a value of `0.8`, at test time, feed it a value of `1.0`. You can explore different dropout values.\n",
    "\n",
    "If dropout works, your model should overfit less, but should still perform about as well (or, hopefully, better) on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_classifier:\n",
    "    def __init__(self, dim):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.3  # Should be about right\n",
    "        self.training_epochs = 100  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = dim  # The number of features\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        \n",
    "        # TODO: Use these.\n",
    "        self.hidden_layer_sizes = [50, 50]\n",
    "        self.keep_rate = 0.8\n",
    "        \n",
    "        # TODO: Overwrite this section\n",
    "        ### Start of model definition ###\n",
    "        \n",
    "        # Define the inputs\n",
    "        self.x = tf.placeholder(tf.float32, [None, dim])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Define (most of) the model\n",
    "        self.h0 = self.x\n",
    "        for i in self.hidden_layer_sizes:\n",
    "            self.W = tf.Variable(tf.zeros([self.dim, 50]))\n",
    "        self.b0 = tf.Variable(tf.zeros([50]))        \n",
    "        self.logits0 = tf.matmul(self.x, self.W) + self.b\n",
    "        self.h0 = tf.nn.relu(self.logits0)\n",
    "        \n",
    "        self.W1 = tf.Variable(tf.zeros([50, 50]))\n",
    "        self.b1 = tf.Variable(tf.zeros([50]))        \n",
    "        self.h1 = tf.nn.relu_layer(self.h0,self.W1,self.b1)\n",
    "        ### End of model definition ###\n",
    "        \n",
    "        # Define the cost function (here, the exp and sum are built in)\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.h1, labels=self.y))\n",
    "        \n",
    "        # Optionally you could add L2 regularization term\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['vector'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print ('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['vector'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 1 Cost: 0.6710758010546367 Dev acc: 0.63 Train acc: 0.626\n",
      "Epoch: 2 Cost: 0.6450143434383251 Dev acc: 0.702 Train acc: 0.656\n",
      "Epoch: 3 Cost: 0.6272322535514833 Dev acc: 0.714 Train acc: 0.676\n",
      "Epoch: 4 Cost: 0.6136985641938669 Dev acc: 0.72 Train acc: 0.72\n",
      "Epoch: 5 Cost: 0.6019859159434284 Dev acc: 0.736 Train acc: 0.704\n",
      "Epoch: 6 Cost: 0.592659283567358 Dev acc: 0.728 Train acc: 0.732\n",
      "Epoch: 7 Cost: 0.5843140924418414 Dev acc: 0.726 Train acc: 0.762\n",
      "Epoch: 8 Cost: 0.5759609540303549 Dev acc: 0.738 Train acc: 0.756\n",
      "Epoch: 9 Cost: 0.5701166236842119 Dev acc: 0.734 Train acc: 0.722\n",
      "Epoch: 10 Cost: 0.5629824377872327 Dev acc: 0.746 Train acc: 0.762\n",
      "Epoch: 11 Cost: 0.5577958557340833 Dev acc: 0.744 Train acc: 0.76\n",
      "Epoch: 12 Cost: 0.552511939296016 Dev acc: 0.752 Train acc: 0.758\n",
      "Epoch: 13 Cost: 0.5475986710301152 Dev acc: 0.748 Train acc: 0.74\n",
      "Epoch: 14 Cost: 0.5421859577850059 Dev acc: 0.738 Train acc: 0.742\n",
      "Epoch: 15 Cost: 0.5378530720869701 Dev acc: 0.732 Train acc: 0.758\n",
      "Epoch: 16 Cost: 0.5339032941394382 Dev acc: 0.734 Train acc: 0.736\n",
      "Epoch: 17 Cost: 0.5303734651318303 Dev acc: 0.744 Train acc: 0.79\n",
      "Epoch: 18 Cost: 0.5260692051163427 Dev acc: 0.746 Train acc: 0.762\n",
      "Epoch: 19 Cost: 0.522720628314548 Dev acc: 0.756 Train acc: 0.742\n",
      "Epoch: 20 Cost: 0.5192176688600469 Dev acc: 0.748 Train acc: 0.78\n",
      "Epoch: 21 Cost: 0.51588370402654 Dev acc: 0.746 Train acc: 0.764\n",
      "Epoch: 22 Cost: 0.5139040980074141 Dev acc: 0.742 Train acc: 0.754\n",
      "Epoch: 23 Cost: 0.5105265952922681 Dev acc: 0.758 Train acc: 0.77\n",
      "Epoch: 24 Cost: 0.507938779062695 Dev acc: 0.754 Train acc: 0.776\n",
      "Epoch: 25 Cost: 0.504749991275646 Dev acc: 0.758 Train acc: 0.778\n",
      "Epoch: 26 Cost: 0.5025177796681723 Dev acc: 0.762 Train acc: 0.766\n",
      "Epoch: 27 Cost: 0.4998051292366451 Dev acc: 0.76 Train acc: 0.798\n",
      "Epoch: 28 Cost: 0.4977916015519036 Dev acc: 0.756 Train acc: 0.808\n",
      "Epoch: 29 Cost: 0.4951054685645633 Dev acc: 0.766 Train acc: 0.782\n",
      "Epoch: 30 Cost: 0.4929242663913302 Dev acc: 0.766 Train acc: 0.802\n",
      "Epoch: 31 Cost: 0.49007975061734516 Dev acc: 0.758 Train acc: 0.776\n",
      "Epoch: 32 Cost: 0.4890655290197442 Dev acc: 0.756 Train acc: 0.746\n",
      "Epoch: 33 Cost: 0.4867514151114004 Dev acc: 0.76 Train acc: 0.794\n",
      "Epoch: 34 Cost: 0.4856919977400039 Dev acc: 0.762 Train acc: 0.772\n",
      "Epoch: 35 Cost: 0.4830147789584267 Dev acc: 0.758 Train acc: 0.772\n",
      "Epoch: 36 Cost: 0.4806878125226056 Dev acc: 0.758 Train acc: 0.79\n",
      "Epoch: 37 Cost: 0.4793736934661864 Dev acc: 0.756 Train acc: 0.786\n",
      "Epoch: 38 Cost: 0.47812433816768507 Dev acc: 0.758 Train acc: 0.772\n",
      "Epoch: 39 Cost: 0.47590571641921986 Dev acc: 0.762 Train acc: 0.784\n",
      "Epoch: 40 Cost: 0.47571598821216154 Dev acc: 0.756 Train acc: 0.798\n",
      "Epoch: 41 Cost: 0.4730603750105257 Dev acc: 0.76 Train acc: 0.806\n",
      "Epoch: 42 Cost: 0.47161243248868867 Dev acc: 0.758 Train acc: 0.788\n",
      "Epoch: 43 Cost: 0.4700477564776385 Dev acc: 0.76 Train acc: 0.762\n",
      "Epoch: 44 Cost: 0.46816634359183135 Dev acc: 0.764 Train acc: 0.784\n",
      "Epoch: 45 Cost: 0.46754239334000497 Dev acc: 0.756 Train acc: 0.788\n",
      "Epoch: 46 Cost: 0.4656890796290504 Dev acc: 0.774 Train acc: 0.814\n",
      "Epoch: 47 Cost: 0.46496072301158187 Dev acc: 0.768 Train acc: 0.808\n",
      "Epoch: 48 Cost: 0.4633649289608001 Dev acc: 0.766 Train acc: 0.794\n",
      "Epoch: 49 Cost: 0.46261685314001866 Dev acc: 0.762 Train acc: 0.804\n",
      "Epoch: 50 Cost: 0.46056949319662877 Dev acc: 0.758 Train acc: 0.802\n",
      "Epoch: 51 Cost: 0.4596614042917887 Dev acc: 0.762 Train acc: 0.8\n",
      "Epoch: 52 Cost: 0.45796597224694713 Dev acc: 0.758 Train acc: 0.81\n",
      "Epoch: 53 Cost: 0.4569241051320677 Dev acc: 0.76 Train acc: 0.822\n",
      "Epoch: 54 Cost: 0.45551403253166756 Dev acc: 0.76 Train acc: 0.82\n",
      "Epoch: 55 Cost: 0.4544036134525582 Dev acc: 0.762 Train acc: 0.802\n",
      "Epoch: 56 Cost: 0.4532112490247797 Dev acc: 0.76 Train acc: 0.8\n",
      "Epoch: 57 Cost: 0.45213222282904153 Dev acc: 0.766 Train acc: 0.826\n",
      "Epoch: 58 Cost: 0.45117125356638876 Dev acc: 0.76 Train acc: 0.828\n",
      "Epoch: 59 Cost: 0.44990592974203614 Dev acc: 0.764 Train acc: 0.82\n",
      "Epoch: 60 Cost: 0.44979266656769645 Dev acc: 0.768 Train acc: 0.796\n",
      "Epoch: 61 Cost: 0.44825087432508115 Dev acc: 0.766 Train acc: 0.808\n",
      "Epoch: 62 Cost: 0.44729863052014956 Dev acc: 0.772 Train acc: 0.81\n",
      "Epoch: 63 Cost: 0.44635165179217295 Dev acc: 0.764 Train acc: 0.786\n",
      "Epoch: 64 Cost: 0.445836117974034 Dev acc: 0.768 Train acc: 0.816\n",
      "Epoch: 65 Cost: 0.4447144016071602 Dev acc: 0.766 Train acc: 0.826\n",
      "Epoch: 66 Cost: 0.44363116997259633 Dev acc: 0.766 Train acc: 0.782\n",
      "Epoch: 67 Cost: 0.443191017265673 Dev acc: 0.77 Train acc: 0.794\n",
      "Epoch: 68 Cost: 0.44146894967114486 Dev acc: 0.764 Train acc: 0.792\n",
      "Epoch: 69 Cost: 0.4409648224159525 Dev acc: 0.77 Train acc: 0.816\n",
      "Epoch: 70 Cost: 0.43967230562810544 Dev acc: 0.766 Train acc: 0.798\n",
      "Epoch: 71 Cost: 0.4392723452161859 Dev acc: 0.764 Train acc: 0.83\n",
      "Epoch: 72 Cost: 0.4389059510495927 Dev acc: 0.764 Train acc: 0.84\n",
      "Epoch: 73 Cost: 0.43707113906189243 Dev acc: 0.766 Train acc: 0.818\n",
      "Epoch: 74 Cost: 0.4367913261607841 Dev acc: 0.768 Train acc: 0.83\n",
      "Epoch: 75 Cost: 0.43638709849781454 Dev acc: 0.77 Train acc: 0.794\n",
      "Epoch: 76 Cost: 0.43583758451320503 Dev acc: 0.766 Train acc: 0.816\n",
      "Epoch: 77 Cost: 0.43443913040337734 Dev acc: 0.762 Train acc: 0.842\n",
      "Epoch: 78 Cost: 0.43402406352537637 Dev acc: 0.768 Train acc: 0.82\n",
      "Epoch: 79 Cost: 0.43301448667490927 Dev acc: 0.768 Train acc: 0.802\n",
      "Epoch: 80 Cost: 0.4325918400729145 Dev acc: 0.768 Train acc: 0.84\n",
      "Epoch: 81 Cost: 0.4320464619883785 Dev acc: 0.764 Train acc: 0.8\n",
      "Epoch: 82 Cost: 0.4310993066540471 Dev acc: 0.772 Train acc: 0.842\n",
      "Epoch: 83 Cost: 0.43007439485302673 Dev acc: 0.764 Train acc: 0.798\n",
      "Epoch: 84 Cost: 0.42995814151234096 Dev acc: 0.768 Train acc: 0.836\n",
      "Epoch: 85 Cost: 0.42890865162566866 Dev acc: 0.764 Train acc: 0.774\n",
      "Epoch: 86 Cost: 0.42827746492845054 Dev acc: 0.76 Train acc: 0.806\n",
      "Epoch: 87 Cost: 0.4282411336898803 Dev acc: 0.766 Train acc: 0.816\n",
      "Epoch: 88 Cost: 0.42762258428114436 Dev acc: 0.766 Train acc: 0.834\n",
      "Epoch: 89 Cost: 0.42643943649751165 Dev acc: 0.762 Train acc: 0.798\n",
      "Epoch: 90 Cost: 0.4261033965481652 Dev acc: 0.764 Train acc: 0.816\n",
      "Epoch: 91 Cost: 0.42538213840237377 Dev acc: 0.77 Train acc: 0.854\n",
      "Epoch: 92 Cost: 0.42451216666786756 Dev acc: 0.766 Train acc: 0.82\n",
      "Epoch: 93 Cost: 0.4239796764320798 Dev acc: 0.762 Train acc: 0.802\n",
      "Epoch: 94 Cost: 0.4234350036691737 Dev acc: 0.758 Train acc: 0.824\n",
      "Epoch: 95 Cost: 0.4228050289330658 Dev acc: 0.758 Train acc: 0.828\n",
      "Epoch: 96 Cost: 0.42204766361801715 Dev acc: 0.76 Train acc: 0.82\n",
      "Epoch: 97 Cost: 0.42182052908120327 Dev acc: 0.764 Train acc: 0.82\n",
      "Epoch: 98 Cost: 0.42156825352598126 Dev acc: 0.76 Train acc: 0.828\n",
      "Epoch: 99 Cost: 0.4209553798039754 Dev acc: 0.764 Train acc: 0.828\n",
      "Epoch: 100 Cost: 0.4202842701364446 Dev acc: 0.764 Train acc: 0.816\n"
     ]
    }
   ],
   "source": [
    "classifier = logistic_regression_classifier(dim)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier(classifier.classify, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
