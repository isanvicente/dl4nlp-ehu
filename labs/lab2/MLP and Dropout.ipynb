{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: MLPs and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6920\n",
      "Dev size: 872\n",
      "Test size: 1821\n"
     ]
    }
   ],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt') \n",
    "\n",
    "print('Training size: {}'.format(len(training_set)))\n",
    "print('Dev size: {}'.format(len(dev_set)))\n",
    "print('Test size: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 10 times in the training set, leaving us with $|V|=1254$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1254\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "                                \n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            # Extract features (by name) for one example\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features, dim\n",
    "    \n",
    "indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n",
    "\n",
    "print('Vocabulary size: {}'.format(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a batch evalution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow.\n",
    "\n",
    "### Part One:\n",
    "\n",
    "Modify it to turn it into an MLP with two ReLU hidden layers of 50 dimensions.\n",
    "\n",
    "Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. (Why?) You should use tf.random_normal instead, with stddev=0.1.\n",
    "\n",
    "If your model works, it should be able to overfit, reaching about 90% accuracy *on the training set* in the first 100 epochs.\n",
    "\n",
    "### Part Two:\n",
    "\n",
    "After each hidden layer, add dropout with a 80% keep rate. You're welcome to use `tf.nn.dropout`.\n",
    "\n",
    "Remember that dropout behaves differently at training time and at test time. This is not automatic. You can implement in various ways, but an easy way can be this:\n",
    "\n",
    "- Hint: Treat the keep rate as an input to the model, just like `x`. At training time, feed it a value of `0.8`, at test time, feed it a value of `1.0`. You can explore different dropout values.\n",
    "\n",
    "If dropout works, your model should overfit less, but should still perform about as well (or, hopefully, better) on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_classifier:\n",
    "    def __init__(self, dim, classes=2, keep_rate=0.8):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.3  # Should be about right\n",
    "        self.training_epochs = 100  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = dim  # The number of features\n",
    "        self.outclasses = classes  # The number of features\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        \n",
    "        # TODO: Use these.\n",
    "        self.hidden_layer_sizes = [50, 50]\n",
    "        self.keep_rate = keep_rate\n",
    "        \n",
    "        # TODO: Overwrite this section\n",
    "        ### Start of model definition ###\n",
    "        \n",
    "        # Define the inputs\n",
    "        self.x = tf.placeholder(tf.float32, [None, dim])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Define (most of) the model\n",
    "        #layer1\n",
    "        self.W0 = tf.Variable(tf.random_normal([self.dim, self.hidden_layer_sizes[0]],stddev=0.1))\n",
    "        self.b0 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[0]],stddev=0.1))        \n",
    "        self.logits0 = tf.matmul(self.x, self.W0) + self.b0\n",
    "        self.h0 = tf.nn.dropout(tf.nn.relu(self.logits0),self.keep_rate)\n",
    "        \n",
    "        #layer2\n",
    "        self.W1 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[0], self.hidden_layer_sizes[1]],stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[1]],stddev=0.1))        \n",
    "        self.h1 = tf.nn.dropout(tf.nn.relu_layer(self.h0,self.W1,self.b1),self.keep_rate)\n",
    "        \n",
    "        #output\n",
    "        self.W2 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[1], self.outclasses],stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.random_normal([self.outclasses],stddev=0.1))\n",
    "        self.logitsh2 = tf.matmul(self.h1, self.W2) + self.b2 \n",
    "                \n",
    "        \n",
    "        ### End of model definition ###\n",
    "        \n",
    "        # Define the cost function (here, the exp and sum are built in)\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logitsh2, labels=self.y))\n",
    "        \n",
    "        # Optionally you could add L2 regularization term\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['vector'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print ('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['vector'] for example in examples])\n",
    "        logits = self.sess.run(self.logitsh2, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 1 Cost: 0.6902036247430023 Dev acc: 0.554 Train acc: 0.536\n",
      "Epoch: 2 Cost: 0.6871951862617776 Dev acc: 0.56 Train acc: 0.568\n",
      "Epoch: 3 Cost: 0.6816903750101726 Dev acc: 0.606 Train acc: 0.57\n",
      "Epoch: 4 Cost: 0.675979495048523 Dev acc: 0.574 Train acc: 0.582\n",
      "Epoch: 5 Cost: 0.6670375091058236 Dev acc: 0.616 Train acc: 0.608\n",
      "Epoch: 6 Cost: 0.6625625292460123 Dev acc: 0.618 Train acc: 0.584\n",
      "Epoch: 7 Cost: 0.6534682887571832 Dev acc: 0.628 Train acc: 0.638\n",
      "Epoch: 8 Cost: 0.6445103972046463 Dev acc: 0.648 Train acc: 0.658\n",
      "Epoch: 9 Cost: 0.6287394916569744 Dev acc: 0.642 Train acc: 0.644\n",
      "Epoch: 10 Cost: 0.621110615906892 Dev acc: 0.636 Train acc: 0.644\n",
      "Epoch: 11 Cost: 0.6115602762610824 Dev acc: 0.65 Train acc: 0.664\n",
      "Epoch: 12 Cost: 0.5979635803787797 Dev acc: 0.67 Train acc: 0.676\n",
      "Epoch: 13 Cost: 0.5768053045979252 Dev acc: 0.67 Train acc: 0.668\n",
      "Epoch: 14 Cost: 0.56851053458673 Dev acc: 0.666 Train acc: 0.682\n",
      "Epoch: 15 Cost: 0.5550680756568909 Dev acc: 0.7 Train acc: 0.722\n",
      "Epoch: 16 Cost: 0.5409329555652761 Dev acc: 0.698 Train acc: 0.754\n",
      "Epoch: 17 Cost: 0.5282634143476134 Dev acc: 0.71 Train acc: 0.706\n",
      "Epoch: 18 Cost: 0.5092238154676225 Dev acc: 0.696 Train acc: 0.71\n",
      "Epoch: 19 Cost: 0.5048164040954025 Dev acc: 0.694 Train acc: 0.78\n",
      "Epoch: 20 Cost: 0.5063105280752535 Dev acc: 0.682 Train acc: 0.798\n",
      "Epoch: 21 Cost: 0.48007098833719886 Dev acc: 0.68 Train acc: 0.75\n",
      "Epoch: 22 Cost: 0.48183695033744534 Dev acc: 0.708 Train acc: 0.8\n",
      "Epoch: 23 Cost: 0.48107197880744945 Dev acc: 0.702 Train acc: 0.758\n",
      "Epoch: 24 Cost: 0.4681495063834719 Dev acc: 0.706 Train acc: 0.762\n",
      "Epoch: 25 Cost: 0.45526886317465043 Dev acc: 0.734 Train acc: 0.83\n",
      "Epoch: 26 Cost: 0.430580563015408 Dev acc: 0.7 Train acc: 0.766\n",
      "Epoch: 27 Cost: 0.44145740844585274 Dev acc: 0.732 Train acc: 0.846\n",
      "Epoch: 28 Cost: 0.44077906122914057 Dev acc: 0.726 Train acc: 0.822\n",
      "Epoch: 29 Cost: 0.4188737516049985 Dev acc: 0.716 Train acc: 0.812\n",
      "Epoch: 30 Cost: 0.41273914443122006 Dev acc: 0.69 Train acc: 0.766\n",
      "Epoch: 31 Cost: 0.4078655540943146 Dev acc: 0.726 Train acc: 0.86\n",
      "Epoch: 32 Cost: 0.39939682903113183 Dev acc: 0.748 Train acc: 0.862\n",
      "Epoch: 33 Cost: 0.38166886236932535 Dev acc: 0.726 Train acc: 0.8\n",
      "Epoch: 34 Cost: 0.39613920450210566 Dev acc: 0.698 Train acc: 0.84\n",
      "Epoch: 35 Cost: 0.3666150084248296 Dev acc: 0.738 Train acc: 0.858\n",
      "Epoch: 36 Cost: 0.35612698837562845 Dev acc: 0.736 Train acc: 0.874\n",
      "Epoch: 37 Cost: 0.34707213883046745 Dev acc: 0.724 Train acc: 0.858\n",
      "Epoch: 38 Cost: 0.33530847673062925 Dev acc: 0.742 Train acc: 0.882\n",
      "Epoch: 39 Cost: 0.36245666830628004 Dev acc: 0.716 Train acc: 0.822\n",
      "Epoch: 40 Cost: 0.33527874725836293 Dev acc: 0.722 Train acc: 0.848\n",
      "Epoch: 41 Cost: 0.3532326089011298 Dev acc: 0.672 Train acc: 0.806\n",
      "Epoch: 42 Cost: 0.3040970282422171 Dev acc: 0.612 Train acc: 0.7\n",
      "Epoch: 43 Cost: 0.33345325787862146 Dev acc: 0.73 Train acc: 0.858\n",
      "Epoch: 44 Cost: 0.322102779591525 Dev acc: 0.72 Train acc: 0.89\n",
      "Epoch: 45 Cost: 0.2892191669455281 Dev acc: 0.636 Train acc: 0.674\n",
      "Epoch: 46 Cost: 0.31190280340335985 Dev acc: 0.662 Train acc: 0.808\n",
      "Epoch: 47 Cost: 0.291594010812265 Dev acc: 0.762 Train acc: 0.908\n",
      "Epoch: 48 Cost: 0.2736831230145914 Dev acc: 0.73 Train acc: 0.882\n",
      "Epoch: 49 Cost: 0.2705671207772361 Dev acc: 0.722 Train acc: 0.918\n",
      "Epoch: 50 Cost: 0.2539542543667334 Dev acc: 0.674 Train acc: 0.866\n",
      "Epoch: 51 Cost: 0.28497970104217524 Dev acc: 0.668 Train acc: 0.802\n",
      "Epoch: 52 Cost: 0.27187659398273184 Dev acc: 0.724 Train acc: 0.872\n",
      "Epoch: 53 Cost: 0.2372181492823142 Dev acc: 0.698 Train acc: 0.934\n",
      "Epoch: 54 Cost: 0.25540908508830606 Dev acc: 0.694 Train acc: 0.858\n",
      "Epoch: 55 Cost: 0.2021097927181809 Dev acc: 0.718 Train acc: 0.918\n",
      "Epoch: 56 Cost: 0.18964963295945414 Dev acc: 0.73 Train acc: 0.942\n",
      "Epoch: 57 Cost: 0.24845928560804437 Dev acc: 0.724 Train acc: 0.928\n",
      "Epoch: 58 Cost: 0.27127486246603505 Dev acc: 0.72 Train acc: 0.942\n",
      "Epoch: 59 Cost: 0.17567264868153468 Dev acc: 0.682 Train acc: 0.914\n",
      "Epoch: 60 Cost: 0.14342790014213985 Dev acc: 0.742 Train acc: 0.946\n",
      "Epoch: 61 Cost: 0.2734400519618282 Dev acc: 0.688 Train acc: 0.894\n",
      "Epoch: 62 Cost: 0.25566935097729715 Dev acc: 0.686 Train acc: 0.886\n",
      "Epoch: 63 Cost: 0.16769146588113573 Dev acc: 0.738 Train acc: 0.956\n",
      "Epoch: 64 Cost: 0.15147740090334857 Dev acc: 0.734 Train acc: 0.956\n",
      "Epoch: 65 Cost: 0.12201421431921147 Dev acc: 0.73 Train acc: 0.928\n",
      "Epoch: 66 Cost: 0.12043561814007933 Dev acc: 0.726 Train acc: 0.954\n",
      "Epoch: 67 Cost: 0.11909411047343857 Dev acc: 0.73 Train acc: 0.968\n",
      "Epoch: 68 Cost: 0.13044092555840808 Dev acc: 0.734 Train acc: 0.968\n",
      "Epoch: 69 Cost: 0.2262002580143787 Dev acc: 0.702 Train acc: 0.908\n",
      "Epoch: 70 Cost: 0.1341404128405783 Dev acc: 0.736 Train acc: 0.95\n",
      "Epoch: 71 Cost: 0.14147466393532576 Dev acc: 0.692 Train acc: 0.886\n",
      "Epoch: 72 Cost: 0.11244272633835119 Dev acc: 0.724 Train acc: 0.952\n",
      "Epoch: 73 Cost: 0.11161010657195693 Dev acc: 0.724 Train acc: 0.974\n",
      "Epoch: 74 Cost: 0.09248249964029699 Dev acc: 0.718 Train acc: 0.936\n",
      "Epoch: 75 Cost: 0.30414141383436 Dev acc: 0.702 Train acc: 0.938\n",
      "Epoch: 76 Cost: 0.24759629341187298 Dev acc: 0.68 Train acc: 0.91\n",
      "Epoch: 77 Cost: 0.13554881540713487 Dev acc: 0.706 Train acc: 0.94\n",
      "Epoch: 78 Cost: 0.2771429102177973 Dev acc: 0.71 Train acc: 0.938\n",
      "Epoch: 79 Cost: 0.2477220219594461 Dev acc: 0.704 Train acc: 0.916\n",
      "Epoch: 80 Cost: 0.2138406667444441 Dev acc: 0.682 Train acc: 0.804\n",
      "Epoch: 81 Cost: 0.17706221756007937 Dev acc: 0.72 Train acc: 0.946\n",
      "Epoch: 82 Cost: 0.11656326497042621 Dev acc: 0.728 Train acc: 0.96\n",
      "Epoch: 83 Cost: 0.11297317345937093 Dev acc: 0.726 Train acc: 0.952\n",
      "Epoch: 84 Cost: 0.10005014638106031 Dev acc: 0.706 Train acc: 0.964\n",
      "Epoch: 85 Cost: 0.09300108391929554 Dev acc: 0.728 Train acc: 0.962\n",
      "Epoch: 86 Cost: 0.0912320641455827 Dev acc: 0.726 Train acc: 0.968\n",
      "Epoch: 87 Cost: 0.08748474399800656 Dev acc: 0.72 Train acc: 0.978\n",
      "Epoch: 88 Cost: 0.0799468166574284 Dev acc: 0.692 Train acc: 0.964\n",
      "Epoch: 89 Cost: 0.09770159243985459 Dev acc: 0.712 Train acc: 0.964\n",
      "Epoch: 90 Cost: 0.07818623204474094 Dev acc: 0.71 Train acc: 0.984\n",
      "Epoch: 91 Cost: 0.07377291056844924 Dev acc: 0.688 Train acc: 0.958\n",
      "Epoch: 92 Cost: 0.07125739325527793 Dev acc: 0.714 Train acc: 0.972\n",
      "Epoch: 93 Cost: 0.06918554008007051 Dev acc: 0.708 Train acc: 0.976\n",
      "Epoch: 94 Cost: 0.060929699214520266 Dev acc: 0.71 Train acc: 0.97\n",
      "Epoch: 95 Cost: 0.07528979289862846 Dev acc: 0.704 Train acc: 0.958\n",
      "Epoch: 96 Cost: 0.06748409389897629 Dev acc: 0.728 Train acc: 0.974\n",
      "Epoch: 97 Cost: 0.0853986593860167 Dev acc: 0.648 Train acc: 0.76\n",
      "Epoch: 98 Cost: 0.31338555558964065 Dev acc: 0.714 Train acc: 0.962\n",
      "Epoch: 99 Cost: 0.1218928622978705 Dev acc: 0.664 Train acc: 0.914\n",
      "Epoch: 100 Cost: 0.14488296183171098 Dev acc: 0.708 Train acc: 0.966\n"
     ]
    }
   ],
   "source": [
    "classifier = logistic_regression_classifier(dim,keep_rate=0.8)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7488532110091743"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(classifier.classify, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
