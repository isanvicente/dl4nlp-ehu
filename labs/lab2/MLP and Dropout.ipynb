{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: MLPs and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6920\n",
      "Dev size: 872\n",
      "Test size: 1821\n"
     ]
    }
   ],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt') \n",
    "\n",
    "print('Training size: {}'.format(len(training_set)))\n",
    "print('Dev size: {}'.format(len(dev_set)))\n",
    "print('Test size: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 10 times in the training set, leaving us with $|V|=1254$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1254\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "                                \n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            # Extract features (by name) for one example\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features, dim\n",
    "    \n",
    "indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n",
    "\n",
    "print('Vocabulary size: {}'.format(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a batch evalution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow.\n",
    "\n",
    "### Part One:\n",
    "\n",
    "Modify it to turn it into an MLP with two ReLU hidden layers of 50 dimensions.\n",
    "\n",
    "Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. (Why?) You should use tf.random_normal instead, with stddev=0.1.\n",
    "\n",
    "If your model works, it should be able to overfit, reaching about 90% accuracy *on the training set* in the first 100 epochs.\n",
    "\n",
    "### Part Two:\n",
    "\n",
    "After each hidden layer, add dropout with a 80% keep rate. You're welcome to use `tf.nn.dropout`.\n",
    "\n",
    "Remember that dropout behaves differently at training time and at test time. This is not automatic. You can implement in various ways, but an easy way can be this:\n",
    "\n",
    "- Hint: Treat the keep rate as an input to the model, just like `x`. At training time, feed it a value of `0.8`, at test time, feed it a value of `1.0`. You can explore different dropout values.\n",
    "\n",
    "If dropout works, your model should overfit less, but should still perform about as well (or, hopefully, better) on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_classifier:\n",
    "    def __init__(self, dim, classes=2):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.3  # Should be about right\n",
    "        self.training_epochs = 100  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = dim  # The number of features\n",
    "        self.outclasses = classes  # The number of features\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        \n",
    "        # TODO: Use these.\n",
    "        self.hidden_layer_sizes = [50, 50]\n",
    "        #self.keep_rate = keep_rate\n",
    "        \n",
    "        # TODO: Overwrite this section\n",
    "        ### Start of model definition ###\n",
    "        \n",
    "        # Define the inputs\n",
    "        self.x = tf.placeholder(tf.float32, [None, dim])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        self.keep_rate = tf.placeholder(tf.float32, shape=())\n",
    "        \n",
    "        # Define (most of) the model\n",
    "        #layer1\n",
    "        self.W0 = tf.Variable(tf.random_normal([self.dim, self.hidden_layer_sizes[0]],stddev=0.1))\n",
    "        self.b0 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[0]],stddev=0.1))        \n",
    "        self.logits0 = tf.matmul(self.x, self.W0) + self.b0\n",
    "        self.h0 = tf.nn.dropout(tf.nn.relu(self.logits0),self.keep_rate)\n",
    "        \n",
    "        #layer2\n",
    "        self.W1 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[0], self.hidden_layer_sizes[1]],stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[1]],stddev=0.1))        \n",
    "        self.h1 = tf.nn.dropout(tf.nn.relu_layer(self.h0,self.W1,self.b1),self.keep_rate)\n",
    "        \n",
    "        #output\n",
    "        self.W2 = tf.Variable(tf.random_normal([self.hidden_layer_sizes[1], self.outclasses],stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.random_normal([self.outclasses],stddev=0.1))\n",
    "        \n",
    "        self.logitsh2 = tf.matmul(self.h1, self.W2) + self.b2 \n",
    "                \n",
    "        \n",
    "        ### End of model definition ###\n",
    "        \n",
    "        # Define the cost function (here, the exp and sum are built in)\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logitsh2, labels=self.y))\n",
    "        \n",
    "        # Optionally you could add L2 regularization term\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set,keep_rate=1.0):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['vector'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print ('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels,\n",
    "                                                self.keep_rate:keep_rate})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n",
    "    \n",
    "    def classify(self, examples, keep_rate=1.0):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['vector'] for example in examples])\n",
    "        logits = self.sess.run(self.logitsh2, feed_dict={self.x: vectors,\n",
    "                                                            self.keep_rate:keep_rate})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 1 Cost: 0.696899431723 Dev acc: 0.528 Train acc: 0.532\n",
      "Epoch: 2 Cost: 0.689820815016 Dev acc: 0.536 Train acc: 0.494\n",
      "Epoch: 3 Cost: 0.685101416376 Dev acc: 0.598 Train acc: 0.57\n",
      "Epoch: 4 Cost: 0.681895399535 Dev acc: 0.612 Train acc: 0.6\n",
      "Epoch: 5 Cost: 0.67757311353 Dev acc: 0.626 Train acc: 0.548\n",
      "Epoch: 6 Cost: 0.670903199249 Dev acc: 0.632 Train acc: 0.588\n",
      "Epoch: 7 Cost: 0.665237320794 Dev acc: 0.636 Train acc: 0.632\n",
      "Epoch: 8 Cost: 0.656296500453 Dev acc: 0.64 Train acc: 0.622\n",
      "Epoch: 9 Cost: 0.650778183231 Dev acc: 0.638 Train acc: 0.652\n",
      "Epoch: 10 Cost: 0.641750309202 Dev acc: 0.66 Train acc: 0.662\n",
      "Epoch: 11 Cost: 0.629644853097 Dev acc: 0.662 Train acc: 0.706\n",
      "Epoch: 12 Cost: 0.610863617173 Dev acc: 0.666 Train acc: 0.688\n",
      "Epoch: 13 Cost: 0.602286031953 Dev acc: 0.654 Train acc: 0.684\n",
      "Epoch: 14 Cost: 0.59497056184 Dev acc: 0.698 Train acc: 0.748\n",
      "Epoch: 15 Cost: 0.573448808105 Dev acc: 0.696 Train acc: 0.7\n",
      "Epoch: 16 Cost: 0.57342915844 Dev acc: 0.7 Train acc: 0.768\n",
      "Epoch: 17 Cost: 0.543398212504 Dev acc: 0.714 Train acc: 0.768\n",
      "Epoch: 18 Cost: 0.545793457164 Dev acc: 0.716 Train acc: 0.792\n",
      "Epoch: 19 Cost: 0.528791921006 Dev acc: 0.726 Train acc: 0.808\n",
      "Epoch: 20 Cost: 0.536765366793 Dev acc: 0.74 Train acc: 0.796\n",
      "Epoch: 21 Cost: 0.506564351144 Dev acc: 0.694 Train acc: 0.764\n",
      "Epoch: 22 Cost: 0.500780253499 Dev acc: 0.742 Train acc: 0.822\n",
      "Epoch: 23 Cost: 0.478142016464 Dev acc: 0.744 Train acc: 0.824\n",
      "Epoch: 24 Cost: 0.484261076759 Dev acc: 0.744 Train acc: 0.828\n",
      "Epoch: 25 Cost: 0.480846097072 Dev acc: 0.746 Train acc: 0.796\n",
      "Epoch: 26 Cost: 0.462775904823 Dev acc: 0.758 Train acc: 0.826\n",
      "Epoch: 27 Cost: 0.457827374891 Dev acc: 0.76 Train acc: 0.852\n",
      "Epoch: 28 Cost: 0.449140771672 Dev acc: 0.744 Train acc: 0.836\n",
      "Epoch: 29 Cost: 0.438386055054 Dev acc: 0.746 Train acc: 0.824\n",
      "Epoch: 30 Cost: 0.419309495776 Dev acc: 0.766 Train acc: 0.872\n",
      "Epoch: 31 Cost: 0.452199270328 Dev acc: 0.632 Train acc: 0.654\n",
      "Epoch: 32 Cost: 0.436455108501 Dev acc: 0.768 Train acc: 0.884\n",
      "Epoch: 33 Cost: 0.404803844514 Dev acc: 0.764 Train acc: 0.882\n",
      "Epoch: 34 Cost: 0.396619645534 Dev acc: 0.768 Train acc: 0.884\n",
      "Epoch: 35 Cost: 0.392843572078 Dev acc: 0.736 Train acc: 0.864\n",
      "Epoch: 36 Cost: 0.413769737438 Dev acc: 0.77 Train acc: 0.884\n",
      "Epoch: 37 Cost: 0.354114277495 Dev acc: 0.756 Train acc: 0.878\n",
      "Epoch: 38 Cost: 0.376193398679 Dev acc: 0.656 Train acc: 0.758\n",
      "Epoch: 39 Cost: 0.364638669623 Dev acc: 0.7 Train acc: 0.82\n",
      "Epoch: 40 Cost: 0.38374930841 Dev acc: 0.752 Train acc: 0.902\n",
      "Epoch: 41 Cost: 0.345299918343 Dev acc: 0.758 Train acc: 0.912\n",
      "Epoch: 42 Cost: 0.341770100373 Dev acc: 0.668 Train acc: 0.8\n",
      "Epoch: 43 Cost: 0.334483548447 Dev acc: 0.722 Train acc: 0.84\n",
      "Epoch: 44 Cost: 0.319669055718 Dev acc: 0.674 Train acc: 0.79\n",
      "Epoch: 45 Cost: 0.346242665693 Dev acc: 0.754 Train acc: 0.934\n",
      "Epoch: 46 Cost: 0.317709492313 Dev acc: 0.758 Train acc: 0.936\n",
      "Epoch: 47 Cost: 0.310612421897 Dev acc: 0.764 Train acc: 0.928\n",
      "Epoch: 48 Cost: 0.28495698505 Dev acc: 0.74 Train acc: 0.922\n",
      "Epoch: 49 Cost: 0.2743831161 Dev acc: 0.756 Train acc: 0.938\n",
      "Epoch: 50 Cost: 0.336037199254 Dev acc: 0.752 Train acc: 0.962\n",
      "Epoch: 51 Cost: 0.291357171756 Dev acc: 0.754 Train acc: 0.95\n",
      "Epoch: 52 Cost: 0.282964589971 Dev acc: 0.654 Train acc: 0.742\n",
      "Epoch: 53 Cost: 0.257281180885 Dev acc: 0.758 Train acc: 0.93\n",
      "Epoch: 54 Cost: 0.292892174588 Dev acc: 0.726 Train acc: 0.894\n",
      "Epoch: 55 Cost: 0.248384611474 Dev acc: 0.692 Train acc: 0.82\n",
      "Epoch: 56 Cost: 0.281131175933 Dev acc: 0.748 Train acc: 0.948\n",
      "Epoch: 57 Cost: 0.2200584522 Dev acc: 0.744 Train acc: 0.952\n",
      "Epoch: 58 Cost: 0.188709025582 Dev acc: 0.68 Train acc: 0.816\n",
      "Epoch: 59 Cost: 0.288310650322 Dev acc: 0.746 Train acc: 0.95\n",
      "Epoch: 60 Cost: 0.179263887858 Dev acc: 0.748 Train acc: 0.968\n",
      "Epoch: 61 Cost: 0.219047464154 Dev acc: 0.75 Train acc: 0.966\n",
      "Epoch: 62 Cost: 0.191642482524 Dev acc: 0.69 Train acc: 0.842\n",
      "Epoch: 63 Cost: 0.242864085568 Dev acc: 0.754 Train acc: 0.962\n",
      "Epoch: 64 Cost: 0.206238427648 Dev acc: 0.746 Train acc: 0.936\n",
      "Epoch: 65 Cost: 0.255256054578 Dev acc: 0.742 Train acc: 0.952\n",
      "Epoch: 66 Cost: 0.176158518151 Dev acc: 0.744 Train acc: 0.97\n",
      "Epoch: 67 Cost: 0.233312532582 Dev acc: 0.754 Train acc: 0.972\n",
      "Epoch: 68 Cost: 0.137820397262 Dev acc: 0.748 Train acc: 0.974\n",
      "Epoch: 69 Cost: 0.12304391574 Dev acc: 0.756 Train acc: 0.988\n",
      "Epoch: 70 Cost: 0.211122831537 Dev acc: 0.756 Train acc: 0.98\n",
      "Epoch: 71 Cost: 0.142554980737 Dev acc: 0.654 Train acc: 0.794\n",
      "Epoch: 72 Cost: 0.313620804637 Dev acc: 0.744 Train acc: 0.988\n",
      "Epoch: 73 Cost: 0.144808424292 Dev acc: 0.742 Train acc: 0.976\n",
      "Epoch: 74 Cost: 0.165389500282 Dev acc: 0.742 Train acc: 0.984\n",
      "Epoch: 75 Cost: 0.12454811732 Dev acc: 0.736 Train acc: 0.97\n",
      "Epoch: 76 Cost: 0.104820567701 Dev acc: 0.742 Train acc: 0.976\n",
      "Epoch: 77 Cost: 0.237290779198 Dev acc: 0.728 Train acc: 0.928\n",
      "Epoch: 78 Cost: 0.141536759282 Dev acc: 0.746 Train acc: 0.99\n",
      "Epoch: 79 Cost: 0.104745489028 Dev acc: 0.74 Train acc: 0.996\n",
      "Epoch: 80 Cost: 0.202394706508 Dev acc: 0.706 Train acc: 0.964\n",
      "Epoch: 81 Cost: 0.20209833207 Dev acc: 0.73 Train acc: 0.974\n",
      "Epoch: 82 Cost: 0.268670816112 Dev acc: 0.732 Train acc: 0.966\n",
      "Epoch: 83 Cost: 0.134727509209 Dev acc: 0.752 Train acc: 0.98\n",
      "Epoch: 84 Cost: 0.102280480028 Dev acc: 0.72 Train acc: 0.972\n",
      "Epoch: 85 Cost: 0.120339597817 Dev acc: 0.768 Train acc: 0.99\n",
      "Epoch: 86 Cost: 0.100030990938 Dev acc: 0.748 Train acc: 0.98\n",
      "Epoch: 87 Cost: 0.0957028932042 Dev acc: 0.754 Train acc: 0.99\n",
      "Epoch: 88 Cost: 0.0849136350056 Dev acc: 0.75 Train acc: 0.99\n",
      "Epoch: 89 Cost: 0.0859822744021 Dev acc: 0.746 Train acc: 0.988\n",
      "Epoch: 90 Cost: 0.0777314574354 Dev acc: 0.754 Train acc: 0.984\n",
      "Epoch: 91 Cost: 0.255919193918 Dev acc: 0.682 Train acc: 0.87\n",
      "Epoch: 92 Cost: 0.254770510727 Dev acc: 0.724 Train acc: 0.982\n",
      "Epoch: 93 Cost: 0.124945959007 Dev acc: 0.71 Train acc: 0.956\n",
      "Epoch: 94 Cost: 0.254692762814 Dev acc: 0.724 Train acc: 0.974\n",
      "Epoch: 95 Cost: 0.104832356727 Dev acc: 0.738 Train acc: 0.974\n",
      "Epoch: 96 Cost: 0.0874900515709 Dev acc: 0.736 Train acc: 0.98\n",
      "Epoch: 97 Cost: 0.0799172548232 Dev acc: 0.728 Train acc: 0.972\n",
      "Epoch: 98 Cost: 0.0919477012422 Dev acc: 0.738 Train acc: 0.998\n",
      "Epoch: 99 Cost: 0.0721068117354 Dev acc: 0.738 Train acc: 0.992\n",
      "Epoch: 100 Cost: 0.13304974663 Dev acc: 0.744 Train acc: 0.986\n"
     ]
    }
   ],
   "source": [
    "classifier = logistic_regression_classifier(dim)\n",
    "classifier.train(training_set, dev_set, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7385321100917431"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(classifier.classify, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
