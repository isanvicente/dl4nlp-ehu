{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4: Sentiment with GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll convert the RNN sentiment classifier from last time into a **GRU** RNN sentiment classifier. While the small dataset and tiny vocabulary that we're using here (for speed) will limit the performance of the model, it should still do substantially better than the plain RNN.\n",
    "\n",
    "![](http://vignette1.wikia.nocookie.net/despicableme/images/b/ba/Gru.jpg/revision/latest/scale-to-width-down/250?cb=20130711023954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
    "# trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to index vectors.\n",
    "\n",
    "To simplify your implementation, we'll use a fixed unrolling length of 20. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "\n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 20\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = SEQ_LEN - len(token_sequence)\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i >= padding:\n",
    "                    if token_sequence[i - padding] in word_indices:\n",
    "                        index = word_indices[token_sequence[i - padding]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[PADDING]\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'index_sequence': array([   0,    0,    0,    0,    0,    0,    0,    0,    0,  585,  939,\n",
      "          1,    1,  302,    1,    1,  502,    1,  337, 1047], dtype=int32), 'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .'}\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print(training_set[18])\n",
    "print(len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments: Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a solved version of last week's RNN exercise. The only change I've made is to increase the learning rate, since GRUs are less likely to do crazy things during optimization. Your job is to convert it into a GRU model. You should have to:\n",
    "\n",
    "- **TODO1**: Add additional trained parameters.\n",
    "- **TODO2**: Modify the `step()` function.\n",
    "- **TODO3**: Modify L2 regularization to incorporate the new parameters.\n",
    "\n",
    "You shouldn't have to edit anything outside of `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNSentimentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 1.0  # Should be about right\n",
    "        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 5  # How often to test and print out statistics\n",
    "        self.dim = 12  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 8  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.l2_lambda = 0.001\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        \n",
    "        # now this is update gate\n",
    "        self.W_rnn = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn = tf.Variable(tf.random_normal([self.dim], stddev=0.1))                \n",
    "        \n",
    "        # TODO1: Add additional GRU parameters\n",
    "        \n",
    "        #forget gate\n",
    "        self.W_rnn_z = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn_z = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        #reset gate \n",
    "        self.W_rnn_t = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn_t = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "\n",
    "        \n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.dim, 2], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([2], stddev=0.1))\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Define one step of the RNN\n",
    "        # TODO2: Modify the step() function to compute GRU step\n",
    "        def step(x, h_prev):\n",
    "            emb = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb_h_prev = tf.concat([emb, h_prev], 1)\n",
    "            #forget gate\n",
    "            zt=tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_rnn_z)  + self.b_rnn_z)\n",
    "            #reset gate\n",
    "            rt=tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_rnn_t)  + self.b_rnn_t)\n",
    "                        \n",
    "            ht = tf.nn.tanh(tf.matmul(tf.concat([rt * h_prev, emb],1),self.W_rnn)  + self.b_rnn)\n",
    "            \n",
    "            h = (1 - zt) * h_prev + zt * ht\n",
    "            return h\n",
    "        \n",
    "        # Split up the inputs into individual tensors\n",
    "        self.x_slices = tf.split(self.x, self.sequence_length, 1)\n",
    "        \n",
    "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
    "        \n",
    "        h_prev = self.h_zero\n",
    "        \n",
    "        # Unroll the RNN\n",
    "        for t in range(self.sequence_length):\n",
    "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
    "            h_prev = step(x_t, h_prev)\n",
    "        \n",
    "        # Compute the logits using one last linear layer\n",
    "        self.logits = tf.matmul(h_prev, self.W_cl) + self.b_cl\n",
    "\n",
    "        # Define the L2 cost\n",
    "        # TODO3: Modify L2 regularization to incorporate the new parameters.\n",
    "        self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_rnn_z)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_rnn_t)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_cl)) +\n",
    "                                         tf.reduce_sum(tf.square(self.b_rnn)) +\n",
    "                                         tf.reduce_sum(tf.square(self.b_rnn_z)) +\n",
    "                                         tf.reduce_sum(tf.square(self.b_rnn_t)) +\n",
    "                                         tf.reduce_sum(tf.square(self.b_cl)))\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y)\n",
    "                                         + self.l2_cost)\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print ('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. If the GRU is doing what it should, you should reach 74% accuracy within your first 200 epochs—a substantial improvement over the 70% figure we saw last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 5 Cost: 0.697890829157 Dev acc: 0.5546875 Train acc: 0.515625\n",
      "Epoch: 10 Cost: 0.695686269689 Dev acc: 0.5546875 Train acc: 0.60546875\n",
      "Epoch: 15 Cost: 0.694159534242 Dev acc: 0.5546875 Train acc: 0.51953125\n",
      "Epoch: 20 Cost: 0.693850585708 Dev acc: 0.5546875 Train acc: 0.5546875\n",
      "Epoch: 25 Cost: 0.693378640546 Dev acc: 0.5546875 Train acc: 0.5390625\n",
      "Epoch: 30 Cost: 0.692986903367 Dev acc: 0.5546875 Train acc: 0.5390625\n",
      "Epoch: 35 Cost: 0.692858073446 Dev acc: 0.5546875 Train acc: 0.5390625\n",
      "Epoch: 40 Cost: 0.692991918988 Dev acc: 0.5546875 Train acc: 0.54296875\n",
      "Epoch: 45 Cost: 0.69298329839 Dev acc: 0.5546875 Train acc: 0.5390625\n",
      "Epoch: 50 Cost: 0.693066314415 Dev acc: 0.5859375 Train acc: 0.55859375\n",
      "Epoch: 55 Cost: 0.692937431512 Dev acc: 0.4453125 Train acc: 0.48828125\n",
      "Epoch: 60 Cost: 0.692489065506 Dev acc: 0.5625 Train acc: 0.55859375\n",
      "Epoch: 65 Cost: 0.691304162697 Dev acc: 0.56640625 Train acc: 0.57421875\n",
      "Epoch: 70 Cost: 0.690803922989 Dev acc: 0.5625 Train acc: 0.51171875\n",
      "Epoch: 75 Cost: 0.688460912969 Dev acc: 0.5703125 Train acc: 0.48046875\n",
      "Epoch: 80 Cost: 0.682851409471 Dev acc: 0.58203125 Train acc: 0.515625\n",
      "Epoch: 85 Cost: 0.680035107666 Dev acc: 0.5390625 Train acc: 0.55859375\n",
      "Epoch: 90 Cost: 0.678118138402 Dev acc: 0.66015625 Train acc: 0.6328125\n",
      "Epoch: 95 Cost: 0.67384666646 Dev acc: 0.61328125 Train acc: 0.6015625\n",
      "Epoch: 100 Cost: 0.653441605745 Dev acc: 0.69921875 Train acc: 0.65625\n",
      "Epoch: 105 Cost: 0.623418456978 Dev acc: 0.70703125 Train acc: 0.6875\n",
      "Epoch: 110 Cost: 0.577690050558 Dev acc: 0.7265625 Train acc: 0.77734375\n",
      "Epoch: 115 Cost: 0.540752406474 Dev acc: 0.734375 Train acc: 0.8203125\n",
      "Epoch: 120 Cost: 0.531102703677 Dev acc: 0.73828125 Train acc: 0.78125\n",
      "Epoch: 125 Cost: 0.508817586634 Dev acc: 0.76953125 Train acc: 0.77734375\n",
      "Epoch: 130 Cost: 0.484596489756 Dev acc: 0.77734375 Train acc: 0.79296875\n",
      "Epoch: 135 Cost: 0.482962011187 Dev acc: 0.76171875 Train acc: 0.78515625\n",
      "Epoch: 140 Cost: 0.455411108556 Dev acc: 0.78125 Train acc: 0.84375\n",
      "Epoch: 145 Cost: 0.461440590797 Dev acc: 0.80078125 Train acc: 0.83984375\n",
      "Epoch: 150 Cost: 0.42435057958 Dev acc: 0.80078125 Train acc: 0.8671875\n",
      "Epoch: 155 Cost: 0.434123489592 Dev acc: 0.8046875 Train acc: 0.828125\n",
      "Epoch: 160 Cost: 0.40073753507 Dev acc: 0.81640625 Train acc: 0.8671875\n",
      "Epoch: 165 Cost: 0.385364771993 Dev acc: 0.8125 Train acc: 0.87890625\n",
      "Epoch: 170 Cost: 0.400104658471 Dev acc: 0.79296875 Train acc: 0.91796875\n",
      "Epoch: 175 Cost: 0.378402554327 Dev acc: 0.80078125 Train acc: 0.8515625\n",
      "Epoch: 180 Cost: 0.365785702511 Dev acc: 0.78515625 Train acc: 0.8671875\n",
      "Epoch: 185 Cost: 0.342569493585 Dev acc: 0.796875 Train acc: 0.86328125\n",
      "Epoch: 190 Cost: 0.35052755475 Dev acc: 0.80078125 Train acc: 0.94140625\n",
      "Epoch: 195 Cost: 0.349997277613 Dev acc: 0.76953125 Train acc: 0.8359375\n",
      "Epoch: 200 Cost: 0.338323567752 Dev acc: 0.77734375 Train acc: 0.875\n",
      "Epoch: 205 Cost: 0.310877679675 Dev acc: 0.76953125 Train acc: 0.9296875\n",
      "Epoch: 210 Cost: 0.312513201325 Dev acc: 0.76171875 Train acc: 0.90234375\n",
      "Epoch: 215 Cost: 0.296010624479 Dev acc: 0.765625 Train acc: 0.9296875\n",
      "Epoch: 220 Cost: 0.304799720093 Dev acc: 0.765625 Train acc: 0.921875\n",
      "Epoch: 225 Cost: 0.302792719669 Dev acc: 0.76171875 Train acc: 0.92578125\n",
      "Epoch: 230 Cost: 0.282884676699 Dev acc: 0.7734375 Train acc: 0.953125\n",
      "Epoch: 235 Cost: 0.270222246647 Dev acc: 0.73828125 Train acc: 0.88671875\n",
      "Epoch: 240 Cost: 0.272752173521 Dev acc: 0.76171875 Train acc: 0.9375\n",
      "Epoch: 245 Cost: 0.275787073705 Dev acc: 0.74609375 Train acc: 0.94921875\n",
      "Epoch: 250 Cost: 0.276803020526 Dev acc: 0.76953125 Train acc: 0.92578125\n",
      "Epoch: 255 Cost: 0.270339764931 Dev acc: 0.7578125 Train acc: 0.94140625\n",
      "Epoch: 260 Cost: 0.257335066795 Dev acc: 0.7578125 Train acc: 0.9296875\n",
      "Epoch: 265 Cost: 0.250747144222 Dev acc: 0.76171875 Train acc: 0.84765625\n",
      "Epoch: 270 Cost: 0.267343169009 Dev acc: 0.76953125 Train acc: 0.9453125\n",
      "Epoch: 275 Cost: 0.248891796227 Dev acc: 0.7734375 Train acc: 0.97265625\n",
      "Epoch: 280 Cost: 0.275942460254 Dev acc: 0.765625 Train acc: 0.94140625\n",
      "Epoch: 285 Cost: 0.247020919566 Dev acc: 0.76953125 Train acc: 0.9375\n",
      "Epoch: 290 Cost: 0.277553079305 Dev acc: 0.75390625 Train acc: 0.9375\n",
      "Epoch: 295 Cost: 0.2282404872 Dev acc: 0.7578125 Train acc: 0.91015625\n",
      "Epoch: 300 Cost: 0.272913954876 Dev acc: 0.73046875 Train acc: 0.91796875\n",
      "Epoch: 305 Cost: 0.260013540034 Dev acc: 0.75390625 Train acc: 0.89453125\n",
      "Epoch: 310 Cost: 0.254786786106 Dev acc: 0.69921875 Train acc: 0.90625\n",
      "Epoch: 315 Cost: 0.226654254728 Dev acc: 0.73046875 Train acc: 0.93359375\n",
      "Epoch: 320 Cost: 0.220447608718 Dev acc: 0.75390625 Train acc: 0.9609375\n",
      "Epoch: 325 Cost: 0.264794893839 Dev acc: 0.7578125 Train acc: 0.95703125\n",
      "Epoch: 330 Cost: 0.244740975124 Dev acc: 0.7734375 Train acc: 0.97265625\n",
      "Epoch: 335 Cost: 0.25652926498 Dev acc: 0.76171875 Train acc: 0.95703125\n",
      "Epoch: 340 Cost: 0.219551207291 Dev acc: 0.7578125 Train acc: 0.95703125\n",
      "Epoch: 345 Cost: 0.315183555638 Dev acc: 0.7421875 Train acc: 0.9453125\n",
      "Epoch: 350 Cost: 0.236801728054 Dev acc: 0.75 Train acc: 0.9453125\n",
      "Epoch: 355 Cost: 0.25758581029 Dev acc: 0.7265625 Train acc: 0.9609375\n",
      "Epoch: 360 Cost: 0.237916238882 Dev acc: 0.7421875 Train acc: 0.9609375\n",
      "Epoch: 365 Cost: 0.200235561088 Dev acc: 0.7578125 Train acc: 0.9375\n",
      "Epoch: 370 Cost: 0.201257402698 Dev acc: 0.76171875 Train acc: 0.95703125\n",
      "Epoch: 375 Cost: 0.251481333816 Dev acc: 0.76171875 Train acc: 0.96875\n",
      "Epoch: 380 Cost: 0.210261444251 Dev acc: 0.74609375 Train acc: 0.96875\n",
      "Epoch: 385 Cost: 0.250462225742 Dev acc: 0.765625 Train acc: 0.91796875\n",
      "Epoch: 390 Cost: 0.189691411676 Dev acc: 0.75390625 Train acc: 0.95703125\n",
      "Epoch: 395 Cost: 0.206230101762 Dev acc: 0.75 Train acc: 0.953125\n",
      "Epoch: 400 Cost: 0.267601332731 Dev acc: 0.7578125 Train acc: 0.9296875\n",
      "Epoch: 405 Cost: 0.20735825819 Dev acc: 0.6640625 Train acc: 0.77734375\n",
      "Epoch: 410 Cost: 0.211649425604 Dev acc: 0.7421875 Train acc: 0.96484375\n",
      "Epoch: 415 Cost: 0.21359082339 Dev acc: 0.74609375 Train acc: 0.95703125\n",
      "Epoch: 420 Cost: 0.270351429228 Dev acc: 0.75 Train acc: 0.9765625\n",
      "Epoch: 425 Cost: 0.251260884382 Dev acc: 0.7421875 Train acc: 0.9609375\n",
      "Epoch: 430 Cost: 0.227558960517 Dev acc: 0.7578125 Train acc: 0.9765625\n",
      "Epoch: 435 Cost: 0.257910146206 Dev acc: 0.75 Train acc: 0.92578125\n",
      "Epoch: 440 Cost: 0.182869294175 Dev acc: 0.734375 Train acc: 0.96875\n",
      "Epoch: 445 Cost: 0.21900057296 Dev acc: 0.73046875 Train acc: 0.95703125\n",
      "Epoch: 450 Cost: 0.192939504429 Dev acc: 0.7421875 Train acc: 0.96484375\n",
      "Epoch: 455 Cost: 0.182042185355 Dev acc: 0.74609375 Train acc: 0.9765625\n",
      "Epoch: 460 Cost: 0.176706890265 Dev acc: 0.75390625 Train acc: 0.98046875\n",
      "Epoch: 465 Cost: 0.168445493888 Dev acc: 0.75 Train acc: 0.98046875\n",
      "Epoch: 470 Cost: 0.24851784386 Dev acc: 0.734375 Train acc: 0.98046875\n",
      "Epoch: 475 Cost: 0.231115833477 Dev acc: 0.73828125 Train acc: 0.98046875\n",
      "Epoch: 480 Cost: 0.168445342669 Dev acc: 0.73046875 Train acc: 0.98046875\n",
      "Epoch: 485 Cost: 0.241526631293 Dev acc: 0.7265625 Train acc: 0.95703125\n",
      "Epoch: 490 Cost: 0.246810141537 Dev acc: 0.7265625 Train acc: 0.984375\n",
      "Epoch: 495 Cost: 0.229927102173 Dev acc: 0.7109375 Train acc: 0.921875\n",
      "Epoch: 500 Cost: 0.231692161825 Dev acc: 0.74609375 Train acc: 0.90625\n"
     ]
    }
   ],
   "source": [
    "classifier = RNNSentimentClassifier(len(word_indices), 20)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
