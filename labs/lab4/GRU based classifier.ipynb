{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4: Sentiment with GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll convert the RNN sentiment classifier from last time into a **GRU** RNN sentiment classifier. While the small dataset and tiny vocabulary that we're using here (for speed) will limit the performance of the model, it should still do substantially better than the plain RNN.\n",
    "\n",
    "![](http://vignette1.wikia.nocookie.net/despicableme/images/b/ba/Gru.jpg/revision/latest/scale-to-width-down/250?cb=20130711023954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
    "# trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to index vectors.\n",
    "\n",
    "To simplify your implementation, we'll use a fixed unrolling length of 20. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "\n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 20\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = SEQ_LEN - len(token_sequence)\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i >= padding:\n",
    "                    if token_sequence[i - padding] in word_indices:\n",
    "                        index = word_indices[token_sequence[i - padding]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[PADDING]\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'index_sequence': array([   0,    0,    0,    0,    0,    0,    0,    0,    0,  585,  939,\n",
      "          1,    1,  302,    1,    1,  502,    1,  337, 1047], dtype=int32), 'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .'}\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print(training_set[18])\n",
    "print(len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments: Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a solved version of last week's RNN exercise. The only change I've made is to increase the learning rate, since GRUs are less likely to do crazy things during optimization. Your job is to convert it into a GRU model. You should have to:\n",
    "\n",
    "- **TODO1**: Add additional trained parameters.\n",
    "- **TODO2**: Modify the `step()` function.\n",
    "- **TODO3**: Modify L2 regularization to incorporate the new parameters.\n",
    "\n",
    "You shouldn't have to edit anything outside of `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNSentimentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 1.0  # Should be about right\n",
    "        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 5  # How often to test and print out statistics\n",
    "        self.dim = 12  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 8  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.l2_lambda = 0.001\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        \n",
    "        # now this is update gate\n",
    "        self.W_rnn = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn = tf.Variable(tf.random_normal([self.dim], stddev=0.1))                \n",
    "        \n",
    "        # TODO1: Add additional GRU parameters\n",
    "        \n",
    "        #forget gate\n",
    "        self.W_rnn_z = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn_z = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        #reset gate \n",
    "        self.W_rnn_t = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn_t = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "\n",
    "        \n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.dim, 2], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([2], stddev=0.1))\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Define one step of the RNN\n",
    "        # TODO2: Modify the step() function to compute GRU step\n",
    "        def step(x, h_prev):\n",
    "            emb = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb_h_prev = tf.concat([emb, h_prev], 1)\n",
    "            #forget gate\n",
    "            zt=tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_rnn_z)  + self.b_rnn_z)\n",
    "            #reset gate\n",
    "            rt=tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_rnn_t)  + self.b_rnn_t)\n",
    "                        \n",
    "            ht = tf.nn.tanh(tf.matmul(tf.concat([rt * h_prev, emb],1),self.W_rnn)  + self.b_rnn)\n",
    "            \n",
    "            h = (1 - zt) * h_prev + zt * ht\n",
    "            return h\n",
    "        \n",
    "        # Split up the inputs into individual tensors\n",
    "        self.x_slices = tf.split(self.x, self.sequence_length, 1)\n",
    "        \n",
    "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
    "        \n",
    "        h_prev = self.h_zero\n",
    "        \n",
    "        # Unroll the RNN\n",
    "        for t in range(self.sequence_length):\n",
    "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
    "            h_prev = step(x_t, h_prev)\n",
    "        \n",
    "        # Compute the logits using one last linear layer\n",
    "        self.logits = tf.matmul(h_prev, self.W_cl) + self.b_cl\n",
    "\n",
    "        # Define the L2 cost\n",
    "        # TODO3: Modify L2 regularization to incorporate the new parameters.\n",
    "        self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_rnn_z)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_rnn_t)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_cl)))\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y)\n",
    "                                         + self.l2_cost)\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print ('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. If the GRU is doing what it should, you should reach 74% accuracy within your first 200 epochsâ€”a substantial improvement over the 70% figure we saw last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 5 Cost: 0.697611418035 Dev acc: 0.5546875 Train acc: 0.57421875\n",
      "Epoch: 10 Cost: 0.695471816593 Dev acc: 0.4453125 Train acc: 0.4140625\n",
      "Epoch: 15 Cost: 0.694087476642 Dev acc: 0.5546875 Train acc: 0.5078125\n",
      "Epoch: 20 Cost: 0.693471665736 Dev acc: 0.4453125 Train acc: 0.4453125\n",
      "Epoch: 25 Cost: 0.694013975285 Dev acc: 0.5546875 Train acc: 0.48046875\n",
      "Epoch: 30 Cost: 0.693087328363 Dev acc: 0.5546875 Train acc: 0.5234375\n",
      "Epoch: 35 Cost: 0.693310351283 Dev acc: 0.5546875 Train acc: 0.55078125\n",
      "Epoch: 40 Cost: 0.692963450043 Dev acc: 0.4453125 Train acc: 0.50390625\n",
      "Epoch: 45 Cost: 0.692827920119 Dev acc: 0.5546875 Train acc: 0.53515625\n",
      "Epoch: 50 Cost: 0.692703249278 Dev acc: 0.5546875 Train acc: 0.515625\n",
      "Epoch: 55 Cost: 0.692812449402 Dev acc: 0.5546875 Train acc: 0.546875\n",
      "Epoch: 60 Cost: 0.692467557059 Dev acc: 0.5546875 Train acc: 0.5\n",
      "Epoch: 65 Cost: 0.692833569315 Dev acc: 0.5546875 Train acc: 0.5234375\n",
      "Epoch: 70 Cost: 0.693443742063 Dev acc: 0.5546875 Train acc: 0.578125\n",
      "Epoch: 75 Cost: 0.693119698101 Dev acc: 0.5546875 Train acc: 0.546875\n",
      "Epoch: 80 Cost: 0.692348725266 Dev acc: 0.5546875 Train acc: 0.53125\n",
      "Epoch: 85 Cost: 0.692802413746 Dev acc: 0.5546875 Train acc: 0.5234375\n",
      "Epoch: 90 Cost: 0.692968880689 Dev acc: 0.5546875 Train acc: 0.50390625\n",
      "Epoch: 95 Cost: 0.693500147925 Dev acc: 0.5546875 Train acc: 0.52734375\n",
      "Epoch: 100 Cost: 0.692516459359 Dev acc: 0.5546875 Train acc: 0.53515625\n",
      "Epoch: 105 Cost: 0.692419071992 Dev acc: 0.5546875 Train acc: 0.55859375\n",
      "Epoch: 110 Cost: 0.692542729554 Dev acc: 0.5546875 Train acc: 0.52734375\n",
      "Epoch: 115 Cost: 0.692863828606 Dev acc: 0.5546875 Train acc: 0.484375\n",
      "Epoch: 120 Cost: 0.692894754586 Dev acc: 0.4453125 Train acc: 0.46484375\n",
      "Epoch: 125 Cost: 0.693973706828 Dev acc: 0.5546875 Train acc: 0.47265625\n",
      "Epoch: 130 Cost: 0.693361319877 Dev acc: 0.5546875 Train acc: 0.58203125\n",
      "Epoch: 135 Cost: 0.692828968719 Dev acc: 0.5546875 Train acc: 0.484375\n",
      "Epoch: 140 Cost: 0.692682820338 Dev acc: 0.5546875 Train acc: 0.47265625\n",
      "Epoch: 145 Cost: 0.692690805153 Dev acc: 0.5546875 Train acc: 0.5390625\n",
      "Epoch: 150 Cost: 0.692388746474 Dev acc: 0.5546875 Train acc: 0.53125\n",
      "Epoch: 155 Cost: 0.692240860727 Dev acc: 0.5546875 Train acc: 0.54296875\n",
      "Epoch: 160 Cost: 0.692708701999 Dev acc: 0.5546875 Train acc: 0.46875\n",
      "Epoch: 165 Cost: 0.692652245363 Dev acc: 0.4453125 Train acc: 0.453125\n",
      "Epoch: 170 Cost: 0.6926464019 Dev acc: 0.5546875 Train acc: 0.5078125\n",
      "Epoch: 175 Cost: 0.69277814141 Dev acc: 0.5546875 Train acc: 0.5234375\n",
      "Epoch: 180 Cost: 0.692743500074 Dev acc: 0.5546875 Train acc: 0.52734375\n",
      "Epoch: 185 Cost: 0.692526559035 Dev acc: 0.5546875 Train acc: 0.5\n",
      "Epoch: 190 Cost: 0.692597239106 Dev acc: 0.5546875 Train acc: 0.5390625\n",
      "Epoch: 195 Cost: 0.692879467099 Dev acc: 0.5546875 Train acc: 0.578125\n",
      "Epoch: 200 Cost: 0.692761611055 Dev acc: 0.5546875 Train acc: 0.5546875\n",
      "Epoch: 205 Cost: 0.692577088321 Dev acc: 0.5546875 Train acc: 0.5703125\n",
      "Epoch: 210 Cost: 0.692670868503 Dev acc: 0.4453125 Train acc: 0.46484375\n",
      "Epoch: 215 Cost: 0.69270755185 Dev acc: 0.5546875 Train acc: 0.49609375\n",
      "Epoch: 220 Cost: 0.69332059666 Dev acc: 0.5546875 Train acc: 0.54296875\n",
      "Epoch: 225 Cost: 0.69334602356 Dev acc: 0.5546875 Train acc: 0.4921875\n",
      "Epoch: 230 Cost: 0.692986528079 Dev acc: 0.5546875 Train acc: 0.5\n",
      "Epoch: 235 Cost: 0.692871784722 Dev acc: 0.5546875 Train acc: 0.59765625\n",
      "Epoch: 240 Cost: 0.693267248295 Dev acc: 0.5546875 Train acc: 0.45703125\n",
      "Epoch: 245 Cost: 0.692968732781 Dev acc: 0.5546875 Train acc: 0.546875\n",
      "Epoch: 250 Cost: 0.6930620869 Dev acc: 0.5546875 Train acc: 0.4765625\n",
      "Epoch: 255 Cost: 0.69302984741 Dev acc: 0.5546875 Train acc: 0.51953125\n",
      "Epoch: 260 Cost: 0.693046051043 Dev acc: 0.5546875 Train acc: 0.52734375\n",
      "Epoch: 265 Cost: 0.69209573004 Dev acc: 0.5546875 Train acc: 0.53125\n",
      "Epoch: 270 Cost: 0.693228101289 Dev acc: 0.5546875 Train acc: 0.515625\n",
      "Epoch: 275 Cost: 0.692727614332 Dev acc: 0.5546875 Train acc: 0.515625\n",
      "Epoch: 280 Cost: 0.692743798097 Dev acc: 0.5546875 Train acc: 0.58984375\n",
      "Epoch: 285 Cost: 0.693160006294 Dev acc: 0.5546875 Train acc: 0.55078125\n",
      "Epoch: 290 Cost: 0.693301364228 Dev acc: 0.5546875 Train acc: 0.5\n",
      "Epoch: 295 Cost: 0.693226432359 Dev acc: 0.5546875 Train acc: 0.5703125\n",
      "Epoch: 300 Cost: 0.693321910169 Dev acc: 0.5546875 Train acc: 0.56640625\n",
      "Epoch: 305 Cost: 0.692602290048 Dev acc: 0.5546875 Train acc: 0.4765625\n",
      "Epoch: 310 Cost: 0.693309863408 Dev acc: 0.5546875 Train acc: 0.48828125\n",
      "Epoch: 315 Cost: 0.693444243184 Dev acc: 0.5546875 Train acc: 0.47265625\n",
      "Epoch: 320 Cost: 0.693277303819 Dev acc: 0.5546875 Train acc: 0.55078125\n",
      "Epoch: 325 Cost: 0.693662391769 Dev acc: 0.5546875 Train acc: 0.50390625\n",
      "Epoch: 330 Cost: 0.692670365175 Dev acc: 0.5546875 Train acc: 0.49609375\n",
      "Epoch: 335 Cost: 0.692592258807 Dev acc: 0.5546875 Train acc: 0.55078125\n",
      "Epoch: 340 Cost: 0.693099675355 Dev acc: 0.4453125 Train acc: 0.4609375\n",
      "Epoch: 345 Cost: 0.69312760565 Dev acc: 0.5546875 Train acc: 0.546875\n",
      "Epoch: 350 Cost: 0.693020306252 Dev acc: 0.5546875 Train acc: 0.53125\n",
      "Epoch: 355 Cost: 0.692524101999 Dev acc: 0.5546875 Train acc: 0.45703125\n",
      "Epoch: 360 Cost: 0.692260686998 Dev acc: 0.5546875 Train acc: 0.53125\n",
      "Epoch: 365 Cost: 0.692288910901 Dev acc: 0.5546875 Train acc: 0.51171875\n",
      "Epoch: 370 Cost: 0.691657097251 Dev acc: 0.51953125 Train acc: 0.546875\n",
      "Epoch: 375 Cost: 0.690192772282 Dev acc: 0.5703125 Train acc: 0.55859375\n",
      "Epoch: 380 Cost: 0.686762050346 Dev acc: 0.60546875 Train acc: 0.5546875\n",
      "Epoch: 385 Cost: 0.68396238486 Dev acc: 0.5859375 Train acc: 0.5234375\n",
      "Epoch: 390 Cost: 0.677062231082 Dev acc: 0.64453125 Train acc: 0.671875\n",
      "Epoch: 395 Cost: 0.671240682955 Dev acc: 0.48046875 Train acc: 0.4453125\n",
      "Epoch: 400 Cost: 0.66595011508 Dev acc: 0.66015625 Train acc: 0.5859375\n",
      "Epoch: 405 Cost: 0.616510494992 Dev acc: 0.6328125 Train acc: 0.625\n",
      "Epoch: 410 Cost: 0.571551382542 Dev acc: 0.7109375 Train acc: 0.77734375\n",
      "Epoch: 415 Cost: 0.535177365497 Dev acc: 0.75 Train acc: 0.7890625\n",
      "Epoch: 420 Cost: 0.514909876717 Dev acc: 0.77734375 Train acc: 0.828125\n",
      "Epoch: 425 Cost: 0.490224783067 Dev acc: 0.77734375 Train acc: 0.8359375\n",
      "Epoch: 430 Cost: 0.486898922258 Dev acc: 0.7890625 Train acc: 0.82421875\n",
      "Epoch: 435 Cost: 0.460889088887 Dev acc: 0.8046875 Train acc: 0.78515625\n",
      "Epoch: 440 Cost: 0.449682677234 Dev acc: 0.78125 Train acc: 0.84375\n",
      "Epoch: 445 Cost: 0.416507643682 Dev acc: 0.7890625 Train acc: 0.78515625\n",
      "Epoch: 450 Cost: 0.415124608411 Dev acc: 0.796875 Train acc: 0.875\n",
      "Epoch: 455 Cost: 0.403059484782 Dev acc: 0.80078125 Train acc: 0.8984375\n",
      "Epoch: 460 Cost: 0.390975295394 Dev acc: 0.80859375 Train acc: 0.83203125\n",
      "Epoch: 465 Cost: 0.386841292734 Dev acc: 0.80078125 Train acc: 0.8203125\n",
      "Epoch: 470 Cost: 0.371449455067 Dev acc: 0.796875 Train acc: 0.83203125\n",
      "Epoch: 475 Cost: 0.360591323287 Dev acc: 0.80078125 Train acc: 0.84765625\n",
      "Epoch: 480 Cost: 0.381032842177 Dev acc: 0.8046875 Train acc: 0.89453125\n",
      "Epoch: 485 Cost: 0.348014087589 Dev acc: 0.796875 Train acc: 0.89453125\n",
      "Epoch: 490 Cost: 0.330312438585 Dev acc: 0.7890625 Train acc: 0.921875\n",
      "Epoch: 495 Cost: 0.315530730618 Dev acc: 0.77734375 Train acc: 0.8984375\n",
      "Epoch: 500 Cost: 0.317978667992 Dev acc: 0.7734375 Train acc: 0.90234375\n"
     ]
    }
   ],
   "source": [
    "classifier = RNNSentimentClassifier(len(word_indices), 20)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
