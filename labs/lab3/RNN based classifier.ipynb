{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3: Sentiment, but slower!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement an **RNN-based sentence classifier**. Plain ol' RNNs aren't very good at sentiment classification, and they're very picky about things like learning rates. However, they're the foundation for things like LSTMs, which we'll learn about next week, and which *are* quite useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6920\n",
      "Dev size: 872\n",
      "Test size: 1821\n"
     ]
    }
   ],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "print('Training size: {}'.format(len(training_set)))\n",
    "print('Dev size: {}'.format(len(dev_set)))\n",
    "print('Test size: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to __index vectors__.\n",
    "\n",
    "To simplify your implementation, we'll use a __fixed unrolling length of 20__. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 20\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = SEQ_LEN - len(token_sequence)\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i >= padding:\n",
    "                    if token_sequence[i - padding] in word_indices:\n",
    "                        index = word_indices[token_sequence[i - padding]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[PADDING]\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .', 'index_sequence': array([  0,   0,   0,   0,   0,   0,   0,   0,   0, 422, 958,   1,   1,\n",
      "       682,   1,   1, 318,   1, 363, 509], dtype=int32), 'label': 1}\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print (training_set[18])\n",
    "print (len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments: Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the TODOs in the code below to make RNN work. If it's set up properly, it should reach dev set accuracy of about 0.7 within 500 epochs with the given hyperparameters.\n",
    "\n",
    "You will find 3 TODOs in the code.\n",
    "\n",
    "### TODO 1:\n",
    "\n",
    "- You have to define the RNN parameters (attribute *self.dim* sets dimmension of hidden state). \n",
    "\n",
    "- (Hint) The paremeters take input's embedding (*self.embedding_dim*) and the previous hidden state (*self.dim*) and provides the current hidden state (*self.dim*).\n",
    "\n",
    "### TODO 2:\n",
    "\n",
    "- Write a (very short) Python function that defines one step of an RNN. (Hint) In each step current input and previous hidden states are involved. \n",
    "\n",
    "- Recall from slides: $f(h_{t-1}, p_t) = tanh(W[h_{t-1};p_t])$. Note that input $x$ at time step $t$ is *translated* to its embedding representation. \n",
    "\n",
    "![](./rnn2.png)\n",
    "\n",
    "\n",
    "### TODO 3:\n",
    "\n",
    "- Unroll the RNN using a *for* loop, and obtain the sentence representation with the final hidden state.\n",
    "\n",
    "- (Hint) Note that we are vectorizing the whole minibatch. That is, in each step we are processing all the examples in the batch together in one go. Try to understand the following two code lines:\n",
    "\n",
    "   $\\rightarrow$ ``self.x_slices = tf.split(self.x, self.sequence_length, 1)``\n",
    "   \n",
    "   $\\rightarrow$ ``self.h_zero = tf.zeros([self.batch_size, self.dim])``\n",
    "   \n",
    "- (Hint) It might be a good idea to reshape (tf.reshape) the tensor at step t in a single tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNSentimentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.2  # Should be about right\n",
    "        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 5  # How often to test and print out statistics\n",
    "        self.dim = 24  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 8  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.l2_lambda = 0.001\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        \n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.dim, 2], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([2], stddev=0.1))\n",
    "        \n",
    "        # TODO 1: Define the RNN parameters\n",
    "        self.W_rnn = tf.Variable(tf.random_normal([self.embedding_dim+self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Split up the inputs into individual tensors\n",
    "        self.x_slices = tf.split(self.x, self.sequence_length, 1)\n",
    "    \n",
    "        # Define the start state of the RNN\n",
    "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
    "        \n",
    "        # TODO 2: Write a (very short) Python function that defines one step of an RNN\n",
    "        def step(x, h_prev):\n",
    "            # Add your code here \n",
    "            pt=tf.nn.embedding_lookup(self.E,x)            \n",
    "            concat=tf.concat([h_prev,tf.reshape(pt,[self.batch_size,self.embedding_dim])],1)\n",
    "            h = tf.tanh(tf.matmul(concat, self.W_rnn) + self.b_rnn)  # Broadcasted addition            \n",
    "            return h\n",
    "        \n",
    "        # TODO 3: Unroll the RNN using a for loop, and and obtain the sentence representation with the final hidden state        \n",
    "        current_h=self.h_zero\n",
    "        for i in range(self.sequence_length):\n",
    "            current_h = step(self.x_slices[i], current_h)\n",
    "        sentence_representation = current_h\n",
    "\n",
    "        # Compute the logits using one last linear layer\n",
    "        self.logits = tf.matmul(sentence_representation, self.W_cl) + self.b_cl\n",
    "\n",
    "\n",
    "        # Define the L2 cost\n",
    "        self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_cl)))\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)+self.l2_cost)\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Training.\n",
      "Epoch: 5 Cost: 0.700139891218 Dev acc: 0.5546875 Train acc: 0.5078125\n",
      "Epoch: 10 Cost: 0.69915288466 Dev acc: 0.5546875 Train acc: 0.484375\n",
      "Epoch: 15 Cost: 0.69836516292 Dev acc: 0.5546875 Train acc: 0.5546875\n",
      "Epoch: 20 Cost: 0.697663667025 Dev acc: 0.5546875 Train acc: 0.48828125\n",
      "Epoch: 25 Cost: 0.69692420518 Dev acc: 0.5546875 Train acc: 0.53515625\n",
      "Epoch: 30 Cost: 0.696329328749 Dev acc: 0.56640625 Train acc: 0.52734375\n",
      "Epoch: 35 Cost: 0.695711184431 Dev acc: 0.5546875 Train acc: 0.44921875\n",
      "Epoch: 40 Cost: 0.69532820472 Dev acc: 0.56640625 Train acc: 0.4609375\n",
      "Epoch: 45 Cost: 0.695023874442 Dev acc: 0.56640625 Train acc: 0.54296875\n",
      "Epoch: 50 Cost: 0.694452219539 Dev acc: 0.546875 Train acc: 0.484375\n",
      "Epoch: 55 Cost: 0.694087096938 Dev acc: 0.55078125 Train acc: 0.53515625\n",
      "Epoch: 60 Cost: 0.69383293611 Dev acc: 0.546875 Train acc: 0.49609375\n",
      "Epoch: 65 Cost: 0.693075096166 Dev acc: 0.546875 Train acc: 0.5078125\n",
      "Epoch: 70 Cost: 0.692844454889 Dev acc: 0.546875 Train acc: 0.5546875\n",
      "Epoch: 75 Cost: 0.692403367272 Dev acc: 0.51953125 Train acc: 0.5078125\n",
      "Epoch: 80 Cost: 0.691882813418 Dev acc: 0.5 Train acc: 0.53125\n",
      "Epoch: 85 Cost: 0.691471658371 Dev acc: 0.5078125 Train acc: 0.55078125\n",
      "Epoch: 90 Cost: 0.690846304099 Dev acc: 0.48828125 Train acc: 0.51953125\n",
      "Epoch: 95 Cost: 0.690067324373 Dev acc: 0.546875 Train acc: 0.51953125\n",
      "Epoch: 100 Cost: 0.689675196453 Dev acc: 0.5078125 Train acc: 0.59765625\n",
      "Epoch: 105 Cost: 0.687523638761 Dev acc: 0.55859375 Train acc: 0.53125\n",
      "Epoch: 110 Cost: 0.684738991437 Dev acc: 0.5703125 Train acc: 0.55859375\n",
      "Epoch: 115 Cost: 0.680903768098 Dev acc: 0.5625 Train acc: 0.55859375\n",
      "Epoch: 120 Cost: 0.676019520671 Dev acc: 0.55859375 Train acc: 0.5625\n",
      "Epoch: 125 Cost: 0.668573311082 Dev acc: 0.57421875 Train acc: 0.57421875\n",
      "Epoch: 130 Cost: 0.664307316144 Dev acc: 0.546875 Train acc: 0.62109375\n",
      "Epoch: 135 Cost: 0.65922749484 Dev acc: 0.59375 Train acc: 0.5859375\n",
      "Epoch: 140 Cost: 0.643895628276 Dev acc: 0.6484375 Train acc: 0.71484375\n",
      "Epoch: 145 Cost: 0.646953439271 Dev acc: 0.58203125 Train acc: 0.65234375\n",
      "Epoch: 150 Cost: 0.636027386895 Dev acc: 0.6171875 Train acc: 0.66015625\n",
      "Epoch: 155 Cost: 0.629647873066 Dev acc: 0.65625 Train acc: 0.69921875\n",
      "Epoch: 160 Cost: 0.619281353774 Dev acc: 0.53515625 Train acc: 0.5625\n",
      "Epoch: 165 Cost: 0.616268517794 Dev acc: 0.54296875 Train acc: 0.640625\n",
      "Epoch: 170 Cost: 0.614395055506 Dev acc: 0.6640625 Train acc: 0.6875\n",
      "Epoch: 175 Cost: 0.614711944704 Dev acc: 0.70703125 Train acc: 0.69921875\n",
      "Epoch: 180 Cost: 0.614745100339 Dev acc: 0.6171875 Train acc: 0.66015625\n",
      "Epoch: 185 Cost: 0.6083432149 Dev acc: 0.671875 Train acc: 0.6953125\n",
      "Epoch: 190 Cost: 0.593712140013 Dev acc: 0.70703125 Train acc: 0.7421875\n",
      "Epoch: 195 Cost: 0.597449539988 Dev acc: 0.65234375 Train acc: 0.6328125\n",
      "Epoch: 200 Cost: 0.599411436805 Dev acc: 0.7109375 Train acc: 0.7109375\n",
      "Epoch: 205 Cost: 0.587635004962 Dev acc: 0.55859375 Train acc: 0.609375\n",
      "Epoch: 210 Cost: 0.579139007462 Dev acc: 0.72265625 Train acc: 0.7109375\n",
      "Epoch: 215 Cost: 0.581396480401 Dev acc: 0.5703125 Train acc: 0.671875\n",
      "Epoch: 220 Cost: 0.583815671779 Dev acc: 0.65234375 Train acc: 0.75390625\n",
      "Epoch: 225 Cost: 0.569929112991 Dev acc: 0.65625 Train acc: 0.6875\n",
      "Epoch: 230 Cost: 0.569266440692 Dev acc: 0.66796875 Train acc: 0.68359375\n",
      "Epoch: 235 Cost: 0.567209345323 Dev acc: 0.54296875 Train acc: 0.6171875\n",
      "Epoch: 240 Cost: 0.555469591309 Dev acc: 0.6953125 Train acc: 0.734375\n",
      "Epoch: 245 Cost: 0.540921365773 Dev acc: 0.71875 Train acc: 0.7890625\n",
      "Epoch: 250 Cost: 0.57160004863 Dev acc: 0.72265625 Train acc: 0.7421875\n",
      "Epoch: 255 Cost: 0.565872995942 Dev acc: 0.57421875 Train acc: 0.66796875\n",
      "Epoch: 260 Cost: 0.55633384762 Dev acc: 0.6328125 Train acc: 0.7109375\n",
      "Epoch: 265 Cost: 0.52930180011 Dev acc: 0.65234375 Train acc: 0.7265625\n",
      "Epoch: 270 Cost: 0.53709186448 Dev acc: 0.68359375 Train acc: 0.71875\n",
      "Epoch: 275 Cost: 0.533088198415 Dev acc: 0.68359375 Train acc: 0.76953125\n",
      "Epoch: 280 Cost: 0.543434911304 Dev acc: 0.71484375 Train acc: 0.82421875\n",
      "Epoch: 285 Cost: 0.512294964658 Dev acc: 0.75390625 Train acc: 0.83203125\n",
      "Epoch: 290 Cost: 0.513062663652 Dev acc: 0.60546875 Train acc: 0.71484375\n",
      "Epoch: 295 Cost: 0.540860941013 Dev acc: 0.67578125 Train acc: 0.7265625\n",
      "Epoch: 300 Cost: 0.500684907039 Dev acc: 0.55078125 Train acc: 0.65625\n",
      "Epoch: 305 Cost: 0.534739677553 Dev acc: 0.55859375 Train acc: 0.66796875\n",
      "Epoch: 310 Cost: 0.511873958287 Dev acc: 0.7421875 Train acc: 0.84375\n",
      "Epoch: 315 Cost: 0.485764734171 Dev acc: 0.72265625 Train acc: 0.8046875\n",
      "Epoch: 320 Cost: 0.470264083809 Dev acc: 0.6875 Train acc: 0.76953125\n",
      "Epoch: 325 Cost: 0.479898993616 Dev acc: 0.67578125 Train acc: 0.80078125\n",
      "Epoch: 330 Cost: 0.49722490708 Dev acc: 0.6796875 Train acc: 0.82421875\n",
      "Epoch: 335 Cost: 0.493063650749 Dev acc: 0.70703125 Train acc: 0.78515625\n",
      "Epoch: 340 Cost: 0.465975892765 Dev acc: 0.66015625 Train acc: 0.796875\n",
      "Epoch: 345 Cost: 0.46724715277 Dev acc: 0.671875 Train acc: 0.75390625\n",
      "Epoch: 350 Cost: 0.452799001226 Dev acc: 0.63671875 Train acc: 0.80859375\n",
      "Epoch: 355 Cost: 0.466905972472 Dev acc: 0.73046875 Train acc: 0.8671875\n",
      "Epoch: 360 Cost: 0.456237657203 Dev acc: 0.546875 Train acc: 0.7265625\n",
      "Epoch: 365 Cost: 0.460978857897 Dev acc: 0.7265625 Train acc: 0.8515625\n",
      "Epoch: 370 Cost: 0.458617858313 Dev acc: 0.70703125 Train acc: 0.83203125\n",
      "Epoch: 375 Cost: 0.484995896066 Dev acc: 0.703125 Train acc: 0.82421875\n",
      "Epoch: 380 Cost: 0.465087646687 Dev acc: 0.6484375 Train acc: 0.84375\n",
      "Epoch: 385 Cost: 0.420209258795 Dev acc: 0.64453125 Train acc: 0.77734375\n",
      "Epoch: 390 Cost: 0.433428733437 Dev acc: 0.70703125 Train acc: 0.83984375\n",
      "Epoch: 395 Cost: 0.417795761868 Dev acc: 0.61328125 Train acc: 0.8125\n",
      "Epoch: 400 Cost: 0.413029548195 Dev acc: 0.70703125 Train acc: 0.83984375\n",
      "Epoch: 405 Cost: 0.411285611214 Dev acc: 0.6875 Train acc: 0.86328125\n",
      "Epoch: 410 Cost: 0.417092090404 Dev acc: 0.70703125 Train acc: 0.859375\n",
      "Epoch: 415 Cost: 0.413598495501 Dev acc: 0.6953125 Train acc: 0.82421875\n",
      "Epoch: 420 Cost: 0.379663457473 Dev acc: 0.65234375 Train acc: 0.81640625\n",
      "Epoch: 425 Cost: 0.385485781564 Dev acc: 0.6796875 Train acc: 0.90234375\n",
      "Epoch: 430 Cost: 0.462665447482 Dev acc: 0.66015625 Train acc: 0.8515625\n",
      "Epoch: 435 Cost: 0.435382386049 Dev acc: 0.6640625 Train acc: 0.86328125\n",
      "Epoch: 440 Cost: 0.377579419701 Dev acc: 0.70703125 Train acc: 0.8984375\n",
      "Epoch: 445 Cost: 0.406997551521 Dev acc: 0.58203125 Train acc: 0.828125\n",
      "Epoch: 450 Cost: 0.414752584917 Dev acc: 0.65625 Train acc: 0.859375\n",
      "Epoch: 455 Cost: 0.497650745842 Dev acc: 0.59765625 Train acc: 0.73046875\n",
      "Epoch: 460 Cost: 0.374998488912 Dev acc: 0.6953125 Train acc: 0.890625\n",
      "Epoch: 465 Cost: 0.512350954391 Dev acc: 0.62890625 Train acc: 0.79296875\n",
      "Epoch: 470 Cost: 0.367428488202 Dev acc: 0.63671875 Train acc: 0.8515625\n",
      "Epoch: 475 Cost: 0.398349487119 Dev acc: 0.6640625 Train acc: 0.8515625\n",
      "Epoch: 480 Cost: 0.399331585125 Dev acc: 0.65234375 Train acc: 0.84375\n",
      "Epoch: 485 Cost: 0.336679912276 Dev acc: 0.69140625 Train acc: 0.859375\n",
      "Epoch: 490 Cost: 0.339829168938 Dev acc: 0.68359375 Train acc: 0.828125\n",
      "Epoch: 495 Cost: 0.346543140985 Dev acc: 0.671875 Train acc: 0.90234375\n",
      "Epoch: 500 Cost: 0.320475100367 Dev acc: 0.6953125 Train acc: 0.89453125\n"
     ]
    }
   ],
   "source": [
    "classifier = RNNSentimentClassifier(len(word_indices), 20)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atribution:\n",
    "Adapted by Oier Lopez de Lacalle, based on a notebook by Sam Bowman at NYU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
