{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3: Sentiment, but slower!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement an **RNN-based sentence classifier**. Plain ol' RNNs aren't very good at sentiment classification, and they're very picky about things like learning rates. However, they're the foundation for things like LSTMs, which we'll learn about next week, and which *are* quite useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6920\n",
      "Dev size: 872\n",
      "Test size: 1821\n"
     ]
    }
   ],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "print('Training size: {}'.format(len(training_set)))\n",
    "print('Dev size: {}'.format(len(dev_set)))\n",
    "print('Test size: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to __index vectors__.\n",
    "\n",
    "To simplify your implementation, we'll use a __fixed unrolling length of 20__. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 20\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = SEQ_LEN - len(token_sequence)\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i >= padding:\n",
    "                    if token_sequence[i - padding] in word_indices:\n",
    "                        index = word_indices[token_sequence[i - padding]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[PADDING]\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .', 'index_sequence': array([  0,   0,   0,   0,   0,   0,   0,   0,   0, 422, 958,   1,   1,\n",
      "       682,   1,   1, 318,   1, 363, 509], dtype=int32), 'label': 1}\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print (training_set[18])\n",
    "print (len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments: Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the TODOs in the code below to make RNN work. If it's set up properly, it should reach dev set accuracy of about 0.7 within 500 epochs with the given hyperparameters.\n",
    "\n",
    "You will find 3 TODOs in the code.\n",
    "\n",
    "### TODO 1:\n",
    "\n",
    "- You have to define the RNN parameters (attribute *self.dim* sets dimmension of hidden state). \n",
    "\n",
    "- (Hint) The paremeters take input's embedding (*self.embedding_dim*) and the previous hidden state (*self.dim*) and provides the current hidden state (*self.dim*).\n",
    "\n",
    "### TODO 2:\n",
    "\n",
    "- Write a (very short) Python function that defines one step of an RNN. (Hint) In each step current input and previous hidden states are involved. \n",
    "\n",
    "- Recall from slides: $f(h_{t-1}, p_t) = tanh(W[h_{t-1};p_t])$. Note that input $x$ at time step $t$ is *translated* to its embedding representation. \n",
    "\n",
    "![](./rnn2.png)\n",
    "\n",
    "\n",
    "### TODO 3:\n",
    "\n",
    "- Unroll the RNN using a *for* loop, and obtain the sentence representation with the final hidden state.\n",
    "\n",
    "- (Hint) Note that we are vectorizing the whole minibatch. That is, in each step we are processing all the examples in the batch together in one go. Try to understand the following two code lines:\n",
    "\n",
    "   $\\rightarrow$ ``self.x_slices = tf.split(self.x, self.sequence_length, 1)``\n",
    "   \n",
    "   $\\rightarrow$ ``self.h_zero = tf.zeros([self.batch_size, self.dim])``\n",
    "   \n",
    "- (Hint) It might be a good idea to reshape (tf.reshape) the tensor at step t in a single tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNSentimentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.2  # Should be about right\n",
    "        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 5  # How often to test and print out statistics\n",
    "        self.dim = 24  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 8  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.l2_lambda = 0.001\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        \n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.dim, 2], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([2], stddev=0.1))\n",
    "        \n",
    "        # TODO 1: Define the RNN parameters\n",
    "        self.W_rnn = tf.Variable(tf.random_normal([self.embedding_dim+self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Split up the inputs into individual tensors\n",
    "        self.x_slices = tf.split(self.x, self.sequence_length, 1)\n",
    "    \n",
    "        # Define the start state of the RNN\n",
    "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
    "        \n",
    "        # TODO 2: Write a (very short) Python function that defines one step of an RNN\n",
    "        def step(x, h_prev):\n",
    "            # Add your code here \n",
    "            pt=tf.nn.embedding_lookup(self.E,x)            \n",
    "            concat=tf.concat([h_prev,tf.reshape(pt,[self.batch_size,self.embedding_dim])],1)\n",
    "            h = tf.tanh(tf.matmul(concat, self.W_rnn) + self.b_rnn)  # Broadcasted addition            \n",
    "            return h\n",
    "        \n",
    "        # TODO 3: Unroll the RNN using a for loop, and and obtain the sentence representation with the final hidden state        \n",
    "        current_h=self.h_zero\n",
    "        for i in range(self.sequence_length):\n",
    "            current_h = step(self.x_slices[i], current_h)\n",
    "        sentence_representation = current_h\n",
    "\n",
    "        # Compute the logits using one last linear layer\n",
    "        self.logits = tf.matmul(sentence_representation, self.W_cl) + self.b_cl\n",
    "\n",
    "\n",
    "        # Define the L2 cost\n",
    "        self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n",
    "                                         tf.reduce_sum(tf.square(self.W_cl)))\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)+self.l2_cost)\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Training.\n",
      "Epoch: 5 Cost: 0.699713283115 Dev acc: 0.5546875 Train acc: 0.546875\n",
      "Epoch: 10 Cost: 0.698888924387 Dev acc: 0.55859375 Train acc: 0.5546875\n",
      "Epoch: 15 Cost: 0.698099231278 Dev acc: 0.5546875 Train acc: 0.484375\n",
      "Epoch: 20 Cost: 0.697392282663 Dev acc: 0.5546875 Train acc: 0.546875\n",
      "Epoch: 25 Cost: 0.696857580432 Dev acc: 0.57421875 Train acc: 0.51953125\n",
      "Epoch: 30 Cost: 0.69634200467 Dev acc: 0.56640625 Train acc: 0.578125\n",
      "Epoch: 35 Cost: 0.695950516948 Dev acc: 0.57421875 Train acc: 0.609375\n",
      "Epoch: 40 Cost: 0.695536756957 Dev acc: 0.5703125 Train acc: 0.5546875\n",
      "Epoch: 45 Cost: 0.695036983048 Dev acc: 0.55078125 Train acc: 0.515625\n",
      "Epoch: 50 Cost: 0.694575548172 Dev acc: 0.54296875 Train acc: 0.5390625\n",
      "Epoch: 55 Cost: 0.69418357699 Dev acc: 0.55078125 Train acc: 0.49609375\n",
      "Epoch: 60 Cost: 0.694103861297 Dev acc: 0.54296875 Train acc: 0.5\n",
      "Epoch: 65 Cost: 0.693618867132 Dev acc: 0.546875 Train acc: 0.515625\n",
      "Epoch: 70 Cost: 0.693253506113 Dev acc: 0.54296875 Train acc: 0.5078125\n",
      "Epoch: 75 Cost: 0.692890633036 Dev acc: 0.5390625 Train acc: 0.48046875\n",
      "Epoch: 80 Cost: 0.692777404079 Dev acc: 0.55078125 Train acc: 0.51171875\n",
      "Epoch: 85 Cost: 0.692440386172 Dev acc: 0.54296875 Train acc: 0.58203125\n",
      "Epoch: 90 Cost: 0.692189863435 Dev acc: 0.54296875 Train acc: 0.53515625\n",
      "Epoch: 95 Cost: 0.691837615437 Dev acc: 0.546875 Train acc: 0.53125\n",
      "Epoch: 100 Cost: 0.691841136526 Dev acc: 0.546875 Train acc: 0.55078125\n",
      "Epoch: 105 Cost: 0.691319359673 Dev acc: 0.55078125 Train acc: 0.515625\n",
      "Epoch: 110 Cost: 0.691523000046 Dev acc: 0.546875 Train acc: 0.59375\n",
      "Epoch: 115 Cost: 0.691226303577 Dev acc: 0.546875 Train acc: 0.53125\n",
      "Epoch: 120 Cost: 0.690877260985 Dev acc: 0.546875 Train acc: 0.5390625\n",
      "Epoch: 125 Cost: 0.69063525509 Dev acc: 0.546875 Train acc: 0.59375\n",
      "Epoch: 130 Cost: 0.690411956222 Dev acc: 0.546875 Train acc: 0.48828125\n",
      "Epoch: 135 Cost: 0.690087596575 Dev acc: 0.55078125 Train acc: 0.5078125\n",
      "Epoch: 140 Cost: 0.690179211122 Dev acc: 0.55078125 Train acc: 0.5625\n",
      "Epoch: 145 Cost: 0.689959111037 Dev acc: 0.55078125 Train acc: 0.52734375\n",
      "Epoch: 150 Cost: 0.689371718301 Dev acc: 0.5546875 Train acc: 0.55859375\n",
      "Epoch: 155 Cost: 0.689234945509 Dev acc: 0.5546875 Train acc: 0.484375\n",
      "Epoch: 160 Cost: 0.688914864152 Dev acc: 0.5546875 Train acc: 0.53125\n",
      "Epoch: 165 Cost: 0.688137699057 Dev acc: 0.5546875 Train acc: 0.53125\n",
      "Epoch: 170 Cost: 0.687172593894 Dev acc: 0.5703125 Train acc: 0.515625\n",
      "Epoch: 175 Cost: 0.685740515038 Dev acc: 0.56640625 Train acc: 0.53515625\n",
      "Epoch: 180 Cost: 0.683542068358 Dev acc: 0.56640625 Train acc: 0.55859375\n",
      "Epoch: 185 Cost: 0.679235155936 Dev acc: 0.55859375 Train acc: 0.609375\n",
      "Epoch: 190 Cost: 0.674176019651 Dev acc: 0.5546875 Train acc: 0.51171875\n",
      "Epoch: 195 Cost: 0.667149532724 Dev acc: 0.59375 Train acc: 0.5703125\n",
      "Epoch: 200 Cost: 0.659430662791 Dev acc: 0.59765625 Train acc: 0.609375\n",
      "Epoch: 205 Cost: 0.653237874861 Dev acc: 0.58203125 Train acc: 0.58984375\n",
      "Epoch: 210 Cost: 0.645300907117 Dev acc: 0.57421875 Train acc: 0.5859375\n",
      "Epoch: 215 Cost: 0.639868840023 Dev acc: 0.58984375 Train acc: 0.640625\n",
      "Epoch: 220 Cost: 0.63456254756 Dev acc: 0.57421875 Train acc: 0.61328125\n",
      "Epoch: 225 Cost: 0.633248232029 Dev acc: 0.56640625 Train acc: 0.6484375\n",
      "Epoch: 230 Cost: 0.629768585717 Dev acc: 0.5703125 Train acc: 0.62890625\n",
      "Epoch: 235 Cost: 0.622566642585 Dev acc: 0.5703125 Train acc: 0.63671875\n",
      "Epoch: 240 Cost: 0.616728164532 Dev acc: 0.57421875 Train acc: 0.65625\n",
      "Epoch: 245 Cost: 0.611185899487 Dev acc: 0.5625 Train acc: 0.6640625\n",
      "Epoch: 250 Cost: 0.605636466433 Dev acc: 0.5625 Train acc: 0.64453125\n",
      "Epoch: 255 Cost: 0.603895763556 Dev acc: 0.5859375 Train acc: 0.64453125\n",
      "Epoch: 260 Cost: 0.601328397239 Dev acc: 0.57421875 Train acc: 0.65625\n",
      "Epoch: 265 Cost: 0.59527618576 Dev acc: 0.5703125 Train acc: 0.640625\n",
      "Epoch: 270 Cost: 0.582162395672 Dev acc: 0.578125 Train acc: 0.69921875\n",
      "Epoch: 275 Cost: 0.582124127282 Dev acc: 0.58984375 Train acc: 0.69921875\n",
      "Epoch: 280 Cost: 0.575161095019 Dev acc: 0.56640625 Train acc: 0.70703125\n",
      "Epoch: 285 Cost: 0.569543249077 Dev acc: 0.5703125 Train acc: 0.6640625\n",
      "Epoch: 290 Cost: 0.56225294537 Dev acc: 0.578125 Train acc: 0.71484375\n",
      "Epoch: 295 Cost: 0.562542080879 Dev acc: 0.5859375 Train acc: 0.69140625\n",
      "Epoch: 300 Cost: 0.562408396491 Dev acc: 0.59765625 Train acc: 0.6953125\n",
      "Epoch: 305 Cost: 0.550971883315 Dev acc: 0.5703125 Train acc: 0.70703125\n",
      "Epoch: 310 Cost: 0.547015952843 Dev acc: 0.578125 Train acc: 0.6484375\n",
      "Epoch: 315 Cost: 0.547957795638 Dev acc: 0.58203125 Train acc: 0.74609375\n",
      "Epoch: 320 Cost: 0.543879149137 Dev acc: 0.55859375 Train acc: 0.7109375\n",
      "Epoch: 325 Cost: 0.547075634753 Dev acc: 0.5703125 Train acc: 0.7109375\n",
      "Epoch: 330 Cost: 0.536736582164 Dev acc: 0.57421875 Train acc: 0.76171875\n",
      "Epoch: 335 Cost: 0.528431737864 Dev acc: 0.55859375 Train acc: 0.73828125\n",
      "Epoch: 340 Cost: 0.530945748091 Dev acc: 0.52734375 Train acc: 0.70703125\n",
      "Epoch: 345 Cost: 0.528151049658 Dev acc: 0.5625 Train acc: 0.71484375\n",
      "Epoch: 350 Cost: 0.524273443001 Dev acc: 0.51171875 Train acc: 0.69140625\n",
      "Epoch: 355 Cost: 0.523316043395 Dev acc: 0.58203125 Train acc: 0.75\n",
      "Epoch: 360 Cost: 0.520834040863 Dev acc: 0.59375 Train acc: 0.74609375\n",
      "Epoch: 365 Cost: 0.518697207725 Dev acc: 0.55859375 Train acc: 0.71875\n",
      "Epoch: 370 Cost: 0.515639992776 Dev acc: 0.5390625 Train acc: 0.72265625\n",
      "Epoch: 375 Cost: 0.50024328298 Dev acc: 0.5390625 Train acc: 0.75\n",
      "Epoch: 380 Cost: 0.489332776379 Dev acc: 0.56640625 Train acc: 0.765625\n",
      "Epoch: 385 Cost: 0.489837743618 Dev acc: 0.546875 Train acc: 0.81640625\n",
      "Epoch: 390 Cost: 0.475894975441 Dev acc: 0.53515625 Train acc: 0.765625\n",
      "Epoch: 395 Cost: 0.475692798694 Dev acc: 0.53125 Train acc: 0.76171875\n",
      "Epoch: 400 Cost: 0.473836982692 Dev acc: 0.55078125 Train acc: 0.7734375\n",
      "Epoch: 405 Cost: 0.473936753141 Dev acc: 0.5859375 Train acc: 0.78125\n",
      "Epoch: 410 Cost: 0.465124359837 Dev acc: 0.53125 Train acc: 0.83984375\n",
      "Epoch: 415 Cost: 0.463044943633 Dev acc: 0.53125 Train acc: 0.79296875\n",
      "Epoch: 420 Cost: 0.457982008104 Dev acc: 0.53125 Train acc: 0.7734375\n",
      "Epoch: 425 Cost: 0.445996155341 Dev acc: 0.5390625 Train acc: 0.83984375\n",
      "Epoch: 430 Cost: 0.444759462719 Dev acc: 0.5703125 Train acc: 0.80859375\n",
      "Epoch: 435 Cost: 0.445432927873 Dev acc: 0.5078125 Train acc: 0.78125\n",
      "Epoch: 440 Cost: 0.445841322343 Dev acc: 0.54296875 Train acc: 0.828125\n",
      "Epoch: 445 Cost: 0.432521996675 Dev acc: 0.5234375 Train acc: 0.81640625\n",
      "Epoch: 450 Cost: 0.430500072462 Dev acc: 0.54296875 Train acc: 0.80859375\n",
      "Epoch: 455 Cost: 0.42677275119 Dev acc: 0.53125 Train acc: 0.83984375\n",
      "Epoch: 460 Cost: 0.413198219405 Dev acc: 0.4921875 Train acc: 0.8125\n",
      "Epoch: 465 Cost: 0.422438107155 Dev acc: 0.54296875 Train acc: 0.84765625\n",
      "Epoch: 470 Cost: 0.413248146022 Dev acc: 0.55078125 Train acc: 0.796875\n",
      "Epoch: 475 Cost: 0.399916707366 Dev acc: 0.52734375 Train acc: 0.78125\n",
      "Epoch: 480 Cost: 0.410758348527 Dev acc: 0.5078125 Train acc: 0.8515625\n",
      "Epoch: 485 Cost: 0.395374236283 Dev acc: 0.5546875 Train acc: 0.83984375\n",
      "Epoch: 490 Cost: 0.385213213938 Dev acc: 0.50390625 Train acc: 0.859375\n",
      "Epoch: 495 Cost: 0.389698060574 Dev acc: 0.5 Train acc: 0.85546875\n",
      "Epoch: 500 Cost: 0.380976189066 Dev acc: 0.51171875 Train acc: 0.8359375\n"
     ]
    }
   ],
   "source": [
    "classifier = RNNSentimentClassifier(len(word_indices), 20)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atribution:\n",
    "Adapted by Oier Lopez de Lacalle, based on a notebook by Sam Bowman at NYU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
