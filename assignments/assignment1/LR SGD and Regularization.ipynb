{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1: Logistic Regression, SGD, and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "#print(training_set[0])\n",
    "#print(dev_set)\n",
    "#print(test_set)\n",
    "\n",
    "# Note: Unlike with feature based classifiers, evaluation here should be fast, \n",
    "# and we don't need to trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract bag-of-words feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "                                \n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            # Extract features (by name) for one example\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features, dim\n",
    "    \n",
    "indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define an evalution function. This is a bit different, since it's designed to let us test an entire big batch of examples at once with the classifier, rather than passing them in one by one. (For larger models or larger training sets, this could run out of memory, but it should be fine for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression_classifier:\n",
    "    def __init__(self, dim, lr=1.0, epchs=50, l2b=0.01):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = lr  # Maybe? Let's tune this --> BEST learning rate: 0.9\n",
    "        self.training_epochs = epchs  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = dim  # The number of features\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        \n",
    "        # Define the inputs\n",
    "        self.x = tf.placeholder(tf.float32, [None, dim])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Define (most of) the model\n",
    "        self.W = tf.Variable(tf.zeros([self.dim, 2]))\n",
    "        self.b = tf.Variable(tf.zeros([2]))\n",
    "        self.logits = tf.matmul(self.x, self.W) + self.b\n",
    "\n",
    "        # Define the cost function (here, the exp and sum are built in)\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "        # Loss function with L2 Regularization with beta=0.01\n",
    "        regularizer = tf.nn.l2_loss(self.W)\n",
    "        self.cost = tf.reduce_mean(self.cost+l2b*regularizer) #0.7694954128440367 - 0.7752293577981652\n",
    "        #self.cost = self.cost+l2b*regularizer #0.7568807339449541\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['vector'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print('Training.')\n",
    "        scores={} \n",
    "        scores['train']=[]\n",
    "        scores['dev']=[]\n",
    "        scores['cost']=[]\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                \n",
    "            \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                scores['cost'].add(avg_cost)\n",
    "                scores['train'].add(evaluate_classifier(self.classify, training_set[0:500]))\n",
    "                scores['dev'].add(evaluate_classifier(self.classify, dev_set[0:500]))\n",
    "                print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost,\n",
    "                      \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \n",
    "                      \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['vector'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl4nlp/lib/python3.5/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(title, data, ylim=None, xlabel=\"epochs\", ylabel=\"Accuracy\"):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    \n",
    "        \n",
    "    train_scores=data['train']\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    \n",
    "    train_sizes=len(train_scores)\n",
    "    \n",
    "    test_scores=data['cost']\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    dev_scores=data['dev']\n",
    "    dev_scores_mean = np.mean(dev_scores, axis=1)\n",
    "    dev_scores_std = np.std(dev_scores, axis=1)\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, dev_scores_mean - dev_scores_std,\n",
    "                     dev_scores_mean + dev_scores_std, alpha=0.1, color=\"b\")\n",
    "    \n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, dev_scores_mean, 'o-', color=\"b\",\n",
    "             label=\"Developement score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cost\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 1 Cost: 0.002945405948493216 Dev acc: 0.59 Train acc: 0.55\n",
      "Epoch: 2 Cost: 0.002718387878741379 Dev acc: 0.58 Train acc: 0.556\n",
      "Epoch: 3 Cost: 0.0023952762992983613 Dev acc: 0.746 Train acc: 0.732\n",
      "Epoch: 4 Cost: 0.0022175889893400447 Dev acc: 0.618 Train acc: 0.592\n",
      "Epoch: 5 Cost: 0.0021680397179874556 Dev acc: 0.764 Train acc: 0.774\n",
      "Epoch: 6 Cost: 0.002131407062902495 Dev acc: 0.74 Train acc: 0.784\n",
      "Epoch: 7 Cost: 0.002048887980722443 Dev acc: 0.75 Train acc: 0.798\n",
      "Epoch: 8 Cost: 0.0019487334993081515 Dev acc: 0.766 Train acc: 0.814\n",
      "Epoch: 9 Cost: 0.0019252835212413362 Dev acc: 0.754 Train acc: 0.794\n",
      "Epoch: 10 Cost: 0.0018973434430167634 Dev acc: 0.778 Train acc: 0.864\n",
      "Epoch: 11 Cost: 0.0018348613219266686 Dev acc: 0.784 Train acc: 0.836\n",
      "Epoch: 12 Cost: 0.001878417631680215 Dev acc: 0.76 Train acc: 0.782\n",
      "Epoch: 13 Cost: 0.0018178368530546627 Dev acc: 0.77 Train acc: 0.88\n",
      "Epoch: 14 Cost: 0.0018143515076695213 Dev acc: 0.768 Train acc: 0.846\n",
      "Epoch: 15 Cost: 0.0018479341330627597 Dev acc: 0.786 Train acc: 0.862\n",
      "Epoch: 16 Cost: 0.0017526635072297517 Dev acc: 0.786 Train acc: 0.912\n",
      "Epoch: 17 Cost: 0.0017901963459465787 Dev acc: 0.766 Train acc: 0.848\n",
      "Epoch: 18 Cost: 0.0017378365802061225 Dev acc: 0.788 Train acc: 0.878\n",
      "Epoch: 19 Cost: 0.0017134028581764409 Dev acc: 0.78 Train acc: 0.858\n",
      "Epoch: 20 Cost: 0.0017192003412034224 Dev acc: 0.772 Train acc: 0.838\n",
      "Epoch: 21 Cost: 0.0017024202237802522 Dev acc: 0.78 Train acc: 0.904\n",
      "Epoch: 22 Cost: 0.001701893974785452 Dev acc: 0.768 Train acc: 0.886\n",
      "Epoch: 23 Cost: 0.001680493535887864 Dev acc: 0.788 Train acc: 0.862\n",
      "Epoch: 24 Cost: 0.001692992682499742 Dev acc: 0.782 Train acc: 0.926\n",
      "Epoch: 25 Cost: 0.0016711582257239908 Dev acc: 0.78 Train acc: 0.906\n",
      "Epoch: 26 Cost: 0.0016759614184222846 Dev acc: 0.792 Train acc: 0.89\n",
      "Epoch: 27 Cost: 0.0016546803138529258 Dev acc: 0.79 Train acc: 0.928\n",
      "Epoch: 28 Cost: 0.0016481884349896399 Dev acc: 0.794 Train acc: 0.89\n",
      "Epoch: 29 Cost: 0.001651860023331311 Dev acc: 0.79 Train acc: 0.912\n",
      "Epoch: 30 Cost: 0.0016582247852865194 Dev acc: 0.792 Train acc: 0.87\n",
      "Epoch: 31 Cost: 0.0016564879592301121 Dev acc: 0.792 Train acc: 0.904\n",
      "Epoch: 32 Cost: 0.0016577829374000432 Dev acc: 0.784 Train acc: 0.888\n",
      "Epoch: 33 Cost: 0.0016561381656814507 Dev acc: 0.774 Train acc: 0.882\n",
      "Epoch: 34 Cost: 0.0016475446496365797 Dev acc: 0.784 Train acc: 0.906\n",
      "Epoch: 35 Cost: 0.0016395758958188472 Dev acc: 0.78 Train acc: 0.902\n",
      "Epoch: 36 Cost: 0.0016352809292988642 Dev acc: 0.786 Train acc: 0.922\n",
      "Epoch: 37 Cost: 0.0016252007723475497 Dev acc: 0.786 Train acc: 0.902\n",
      "Epoch: 38 Cost: 0.0016351478924560875 Dev acc: 0.792 Train acc: 0.908\n",
      "Epoch: 39 Cost: 0.0016316261349452865 Dev acc: 0.788 Train acc: 0.906\n",
      "Epoch: 40 Cost: 0.0016268947618772033 Dev acc: 0.776 Train acc: 0.932\n",
      "Epoch: 41 Cost: 0.0016305568170975205 Dev acc: 0.802 Train acc: 0.918\n",
      "Epoch: 42 Cost: 0.0016188503165418904 Dev acc: 0.792 Train acc: 0.934\n",
      "Epoch: 43 Cost: 0.0016178415820899382 Dev acc: 0.794 Train acc: 0.908\n",
      "Epoch: 44 Cost: 0.0016295315516698691 Dev acc: 0.79 Train acc: 0.916\n",
      "Epoch: 45 Cost: 0.001612330295352472 Dev acc: 0.796 Train acc: 0.904\n",
      "Epoch: 46 Cost: 0.0016191091207373473 Dev acc: 0.782 Train acc: 0.912\n",
      "Epoch: 47 Cost: 0.0016232947862051704 Dev acc: 0.792 Train acc: 0.938\n",
      "Epoch: 48 Cost: 0.0016142985680037076 Dev acc: 0.788 Train acc: 0.922\n",
      "Epoch: 49 Cost: 0.0016109706161336764 Dev acc: 0.796 Train acc: 0.928\n",
      "Epoch: 50 Cost: 0.001619660700843842 Dev acc: 0.794 Train acc: 0.908\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    classifier = logistic_regression_classifier(dim, 0.9,50,0.001)\n",
    "    scores = classifier.train(training_set, dev_set)\n",
    "    plot_learning_curve(\"testing plot\",scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7752293577981652"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(classifier.classify, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "### Our goals\n",
    "  1. **Pick an effective learning rate**:\n",
    "      - You could set up the learning rate value by passing it as argument (e.g. in `__init__ (self, dim, lr=1.0, ...)` )\n",
    "      - Try small and larger values to see the behavior of the model.\n",
    "  \n",
    "  2. **Implement L2 regularization:**\n",
    "      - Hint: Add regularization term to overal cost (`self.cost`)\n",
    "      - Tensorflow already built in method for this. Check the API to find out. \n",
    "      - (Optionaly) Code it without using the built in tool for it\n",
    "\n",
    "  3. **Pick an effective L2 weight:**\n",
    "      - You could set up the learning rate value by passing it as argument (e.g. in `__init__ (self, dim, lw=1.0, ...)` )\n",
    "      - Try small and larger values to see the behavior of the model.\n",
    "  \n",
    "  4. **Look at some learning curves:**\n",
    "      - This code might be helpful: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
