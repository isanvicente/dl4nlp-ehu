{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1: Logistic Regression, SGD, and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "#print(training_set[0])\n",
    "#print(dev_set)\n",
    "#print(test_set)\n",
    "\n",
    "# Note: Unlike with feature based classifiers, evaluation here should be fast, \n",
    "# and we don't need to trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract bag-of-words feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "                                \n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            # Extract features (by name) for one example\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features, dim\n",
    "    \n",
    "indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define an evalution function. This is a bit different, since it's designed to let us test an entire big batch of examples at once with the classifier, rather than passing them in one by one. (For larger models or larger training sets, this could run out of memory, but it should be fine for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class logistic_regression_classifier:\n",
    "    def __init__(self, dim, lr=1.0, epchs=50):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = lr  # Maybe? Let's tune this --> BEST learning rate: 0.9\n",
    "        self.training_epochs = epchs  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = dim  # The number of features\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        \n",
    "        # Define the inputs\n",
    "        self.x = tf.placeholder(tf.float32, [None, dim])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Define (most of) the model\n",
    "        self.W = tf.Variable(tf.zeros([self.dim, 2]))\n",
    "        self.b = tf.Variable(tf.zeros([2]))\n",
    "        self.logits = tf.matmul(self.x, self.W) + self.b\n",
    "\n",
    "        # Define the cost function (here, the exp and sum are built in)\n",
    "        self.cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "        # Loss function with L2 Regularization with beta=0.01\n",
    "        regularizer = tf.nn.l2_loss(self.W)\n",
    "        self.cost = self.cost+self.b*regularizer\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['vector'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost,\n",
    "                      \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \n",
    "                      \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        vectors = np.vstack([example['vector'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.x: vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 1 Cost: 0.00310194683779 Dev acc: 0.498 Train acc: 0.506\n",
      "Epoch: 2 Cost: 0.00276696550901 Dev acc: 0.678 Train acc: 0.684\n",
      "Epoch: 3 Cost: 0.00224092538917 Dev acc: 0.61 Train acc: 0.58\n",
      "Epoch: 4 Cost: 0.00229212493394 Dev acc: 0.674 Train acc: 0.682\n",
      "Epoch: 5 Cost: 0.00206224330597 Dev acc: 0.764 Train acc: 0.792\n",
      "Epoch: 6 Cost: 0.00199582357029 Dev acc: 0.758 Train acc: 0.808\n",
      "Epoch: 7 Cost: 0.00187951964068 Dev acc: 0.768 Train acc: 0.82\n",
      "Epoch: 8 Cost: 0.00191835991831 Dev acc: 0.69 Train acc: 0.744\n",
      "Epoch: 9 Cost: 0.00183469878889 Dev acc: 0.762 Train acc: 0.796\n",
      "Epoch: 10 Cost: 0.00172145434017 Dev acc: 0.778 Train acc: 0.822\n",
      "Epoch: 11 Cost: 0.00169320480415 Dev acc: 0.78 Train acc: 0.866\n",
      "Epoch: 12 Cost: 0.00169078079139 Dev acc: 0.728 Train acc: 0.774\n",
      "Epoch: 13 Cost: 0.00166268272463 Dev acc: 0.78 Train acc: 0.842\n",
      "Epoch: 14 Cost: 0.00158987081334 Dev acc: 0.786 Train acc: 0.872\n",
      "Epoch: 15 Cost: 0.00154061483961 Dev acc: 0.788 Train acc: 0.88\n",
      "Epoch: 16 Cost: 0.00151144071154 Dev acc: 0.788 Train acc: 0.868\n",
      "Epoch: 17 Cost: 0.00148829423684 Dev acc: 0.782 Train acc: 0.88\n",
      "Epoch: 18 Cost: 0.00146978587792 Dev acc: 0.782 Train acc: 0.884\n",
      "Epoch: 19 Cost: 0.00143456156797 Dev acc: 0.782 Train acc: 0.872\n",
      "Epoch: 20 Cost: 0.00141296083435 Dev acc: 0.788 Train acc: 0.922\n",
      "Epoch: 21 Cost: 0.00138125202136 Dev acc: 0.792 Train acc: 0.894\n",
      "Epoch: 22 Cost: 0.00137613678817 Dev acc: 0.788 Train acc: 0.886\n",
      "Epoch: 23 Cost: 0.00133864796307 Dev acc: 0.792 Train acc: 0.914\n",
      "Epoch: 24 Cost: 0.00133088424681 Dev acc: 0.78 Train acc: 0.904\n",
      "Epoch: 25 Cost: 0.00132436114708 Dev acc: 0.792 Train acc: 0.908\n",
      "Epoch: 26 Cost: 0.00127971172764 Dev acc: 0.79 Train acc: 0.94\n",
      "Epoch: 27 Cost: 0.00127636039785 Dev acc: 0.784 Train acc: 0.914\n",
      "Epoch: 28 Cost: 0.001250545546 Dev acc: 0.79 Train acc: 0.916\n",
      "Epoch: 29 Cost: 0.00123010227388 Dev acc: 0.796 Train acc: 0.912\n",
      "Epoch: 30 Cost: 0.00121425192682 Dev acc: 0.782 Train acc: 0.906\n",
      "Epoch: 31 Cost: 0.00119865720626 Dev acc: 0.794 Train acc: 0.936\n",
      "Epoch: 32 Cost: 0.00118729858056 Dev acc: 0.796 Train acc: 0.938\n",
      "Epoch: 33 Cost: 0.00116818335718 Dev acc: 0.79 Train acc: 0.906\n",
      "Epoch: 34 Cost: 0.00115942539802 Dev acc: 0.8 Train acc: 0.942\n",
      "Epoch: 35 Cost: 0.00114082065583 Dev acc: 0.788 Train acc: 0.942\n",
      "Epoch: 36 Cost: 0.00113005877507 Dev acc: 0.79 Train acc: 0.94\n",
      "Epoch: 37 Cost: 0.00111261506875 Dev acc: 0.798 Train acc: 0.954\n",
      "Epoch: 38 Cost: 0.00109928530313 Dev acc: 0.804 Train acc: 0.926\n",
      "Epoch: 39 Cost: 0.00109084589941 Dev acc: 0.788 Train acc: 0.944\n",
      "Epoch: 40 Cost: 0.00108237087908 Dev acc: 0.798 Train acc: 0.932\n",
      "Epoch: 41 Cost: 0.00106973602454 Dev acc: 0.794 Train acc: 0.94\n",
      "Epoch: 42 Cost: 0.00105931093545 Dev acc: 0.81 Train acc: 0.94\n",
      "Epoch: 43 Cost: 0.00105192023114 Dev acc: 0.79 Train acc: 0.934\n",
      "Epoch: 44 Cost: 0.00103616893843 Dev acc: 0.784 Train acc: 0.948\n",
      "Epoch: 45 Cost: 0.00102760333188 Dev acc: 0.782 Train acc: 0.948\n",
      "Epoch: 46 Cost: 0.00101555502516 Dev acc: 0.788 Train acc: 0.954\n",
      "Epoch: 47 Cost: 0.00100415875635 Dev acc: 0.806 Train acc: 0.942\n",
      "Epoch: 48 Cost: 0.000997400870633 Dev acc: 0.802 Train acc: 0.934\n",
      "Epoch: 49 Cost: 0.000987743603117 Dev acc: 0.804 Train acc: 0.944\n",
      "Epoch: 50 Cost: 0.000978922766754 Dev acc: 0.788 Train acc: 0.956\n"
     ]
    }
   ],
   "source": [
    "classifier = logistic_regression_classifier(dim, 0.9)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768348623853211"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(classifier.classify, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "### Our goals\n",
    "  1. **Pick an effective learning rate**:\n",
    "      - You could set up the learning rate value by passing it as argument (e.g. in `__init__ (self, dim, lr=1.0, ...)` )\n",
    "      - Try small and larger values to see the behavior of the model.\n",
    "  \n",
    "  2. **Implement L2 regularization:**\n",
    "      - Hint: Add regularization term to overal cost (`self.cost`)\n",
    "      - Tensorflow already built in method for this. Check the API to find out. \n",
    "      - (Optionaly) Code it without using the built in tool for it\n",
    "\n",
    "  3. **Pick an effective L2 weight:**\n",
    "      - You could set up the learning rate value by passing it as argument (e.g. in `__init__ (self, dim, lw=1.0, ...)` )\n",
    "      - Try small and larger values to see the behavior of the model.\n",
    "  \n",
    "  4. **Look at some learning curves:**\n",
    "      - This code might be helpful: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
